---
title: BPç¥ç»ç½‘ç»œç®—æ³•çš„åŸºæœ¬åŸç†åŠC++å®ç°
hide: false
math: true
abbrlink: 2534
date: 2022-01-29 10:48:27
index_img:
banner_img:
category:
 - æ•°å­¦å»ºæ¨¡
tags:
 - ç¥ç»ç½‘ç»œ
---

> é€šè¿‡ä¸‰å¤©çš„æŠ˜è…¾ï¼Œæ€»ç®—æŠŠç¥ç»ç½‘ç»œçš„C++ä»£ç å†™å‡ºæ¥äº†ï¼ˆç”¨C++å†™çº¯å±å› ä¸ºæˆ‘å¯¹å®ƒæ›´ç†Ÿæ‚‰ä¸€äº›ï¼Œè™½ç„¶ç°åœ¨åŸºæœ¬äººå·¥æ™ºèƒ½ç®—æ³•éƒ½æ˜¯Pythonå†™çš„ï¼Œä½†C++å¿«å‘€ğŸ˜†ï¼‰ï¼ˆä¸»è¦è¿˜æ˜¯å¯¹Pythonä¸ç†Ÿç»ƒï¼Œä»¥åæœ‰æ—¶é—´åº”è¯¥ç”¨Pythoné‡å†™ä¸€éï¼‰ï¼Œä»£ç 280è¡Œå·¦å³ï¼ˆå¯¹è‡ªå·±ä¹Ÿæ˜¯ä¸€æ¬¡å¯¹ä»£ç ç†Ÿç»ƒåº¦çš„è®­ç»ƒï¼‰ï¼Œä½¿ç”¨BP(Back Propagation)ç¥ç»ç½‘ç»œï¼Œå­¦ä¹ ç®—æ³•ä¸ºéšæœºæ¢¯åº¦ä¸‹é™æ³•

> æ”¯æŒå¤šçº¿ç¨‹å­¦ä¹ ï¼ˆä¿è¯è·‘æ»¡CPUï¼ŒGPUç®—æ³•è¿˜æ²¡ç ”ç©¶ï¼‰ï¼Œæ”¯æŒå­¦ä¹ ä¸­æ–­ã€ç»§æ‰¿å­¦ä¹ ï¼ˆç¨‹åºè¿è¡Œåˆ°ä¸€åŠå¯ä»¥ç›´æ¥å…³é—­ï¼Œå½“å‰ç½‘ç»œæ•°æ®å‡å·²ä¿å­˜ï¼Œå¯ä½œä¸ºä¸‹æ¬¡å­¦ä¹ å¼€å§‹çš„æ•°æ®ï¼‰

> å…ˆåœ¨è¯†åˆ«æ‰‹å†™æ•°å­—ä¸Šè¿›è¡Œäº†åº”ç”¨ã€‚

å¯¹ç¥ç»ç½‘ç»œçš„ä»‹ç»å’Œç†è§£ï¼Œæˆ‘éƒ½æ˜¯ä» [3B1B - æ·±åº¦å­¦ä¹ ç³»åˆ—è§†é¢‘](https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528929) ä¸­å­¦ä¹ çš„ï¼Œæœ¬æ–‡ä¸­å¾ˆå¤šå›¾ç‰‡ä¹Ÿæ˜¯ä»è¯¥è§†é¢‘ä¸­æˆªå–å‡ºæ¥çš„ï¼ˆä»–åšçš„å›¾ç¤ºæ•ˆæœå¤ªå¥½äº†ğŸ˜ï¼‰ï¼ŒBPç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ æœ¬è´¨æ²¡æœ‰å¾ˆå¤§çš„åŒºåˆ«ï¼Œå°±æ¢ä¸ªåå­—ç½¢äº†ï¼ŒBPçš„å«ä¹‰æ˜¯é€šè¿‡Back Propagationè¿™ä¸ªæ–¹æ³•ï¼Œä¼˜åŒ–æ•´ä¸ªç½‘ç»œå‚æ•°ï¼Œä½¿å¾—æœ€ç»ˆçš„ç»“æœæ›´æ¥è¿‘æˆ‘ä»¬çš„ç›®æ ‡å€¼ã€‚ï¼ˆBack propagationæ–¹æ³•ä¸‹æ–‡ä¼šç»†è®²ï¼‰

æœ¬æ–‡ä¸»è¦ç ”ç©¶æ•°å­¦è®¡ç®—éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯ä¸Šè¿°è§†é¢‘çš„ [Part3 - P2](https://www.bilibili.com/video/BV16x411V7Qg?p=2) éƒ¨åˆ†çš„å†…å®¹ï¼Œæ¨å¯¼å…¬å¼ï¼Œä¹Ÿå°±æ˜¯æ•´ä¸ªç®—æ³•æœ€æ ¸å¿ƒçš„ä¸œè¥¿â€œæ¢¯åº¦ä¸‹é™æ³•â€ã€‚

### ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†

ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œæ˜¯ç”±å¾ˆå¤šå±‚ç»„æˆï¼Œæ¯ä¸€å±‚åˆæœ‰å¾ˆå¤šçš„ç»“ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ï¼Œç›¸é‚»çš„ä¸¤å±‚ä¹‹é—´çš„æ‰€æœ‰ç»“ç‚¹ä¸¤ä¸¤è¿æ¥ï¼ˆè½´çªï¼‰ï¼Œè¿™æ ·ä¸€ä¸ªç¥ç»ç½‘ç»œå°±å»ºæˆäº†ï¼Œå¦‚ä¸‹å›¾ï¼š

è®°ä¸€ä¸ªç¥ç»ç½‘ç»œçš„å±‚æ•°ä¸º $L$ã€‚

![ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ](https://s4.ax1x.com/2022/01/29/HSuMcT.png)

> æ¯”å¦‚è¿™ä¸ªç¥ç»ç½‘ç»œï¼Œ$L = 4$ï¼Œå°±ç”±4å±‚æ„æˆï¼Œåˆ†åˆ«è®°ä¸º $net_1,net_2,net_3,net_4$ï¼Œæ¯ä¸€å±‚ä¸­çš„ç»“ç‚¹æ•°åˆ†åˆ«ä¸º $N_1,N_2,N_3,N_4$ ä¸ªã€‚

å°†ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚ $net_1$ ç§°ä¸º **è¾“å…¥å±‚**ï¼Œ æœ€åä¸€å±‚ $net_L$ ç§°ä¸º **è¾“å‡ºå±‚**ã€‚

#### ç½‘ç»œçš„è®¾è®¡

è¾“å…¥å±‚ï¼šæ ¹æ®è¾“å…¥æ•°æ®çš„è¦æ±‚ï¼Œæ¯”å¦‚ä¸€å¼ æ‰‹å†™æ•°å­—ç…§ç‰‡ï¼Œå°±æ˜¯ç”± $28\times 28$ çš„åƒç´ çŸ©é˜µæ„æˆï¼ŒçŸ©é˜µä¸­æ¯ä¸ªç‚¹åœ¨ $0~255$ ä¹‹é—´è¡¨ç¤ºç°åº¦å€¼ï¼Œé‚£ä¹ˆå¦‚æœå°†è¿™ä¸ªçŸ©é˜µæ‹‰æˆä¸€è¡Œï¼Œå³ $28\cdot 28 = 784$ï¼Œé‚£ä¹ˆå¯¹äºè¯†åˆ«å›¾åƒçš„é—®é¢˜è€Œè¨€ï¼Œè¾“å…¥å±‚å°±è¦æœ‰ $784$ ä¸ªç»“ç‚¹ï¼Œå³ $N_1 = 784$ã€‚

è¾“å‡ºå±‚ï¼šæ ¹æ®è¾“å‡ºæ•°æ®çš„è¦æ±‚ï¼Œæ¯”å¦‚ä¸€å¼ æ‰‹å†™æ•°å­—ï¼Œå¿…å®šä¼šå¯¹åº”ä¸€ä¸ª $0\sim 9$ ä¹‹é—´çš„æ•°å­—ï¼Œé‚£ä¹ˆè¾“å‡ºå±‚å°±è¦åœ¨ $0\sim 9$ ä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œäºæ˜¯è¾“å‡ºå±‚å°±è¦æœ‰ $10$ ä¸ªç»“ç‚¹ï¼Œå³ $N_L = 10$ã€‚

éšå«å±‚ï¼šé™¤å»ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚ï¼Œå³ $2\sim L-1$ è¿™äº›å±‚çš„ç»“ç‚¹ä¸ªæ•°å®Œå…¨ç”±ä½ æ¥å®šï¼Œæ²¡æœ‰å¼ºåˆ¶æ€§è¦æ±‚ï¼Œä½†è‡³å°‘è¦æœ‰ä¸€å±‚ï¼Œå³ $L \geqslant 3$ã€‚

ä¸‹é¢ä»¥æ‰‹å†™æ•°å­—çš„ç¥ç»ç½‘ç»œç»“æ„ä¸ºä¾‹ï¼š

![è¯†åˆ«æ‰‹å†™æ•°å­—çš„ç¥ç»ç½‘ç»œ](https://s4.ax1x.com/2022/01/29/HSKC5R.png)

> åœ¨è¿™ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œ$L = 4,\ N_1=784,\ N_2 = 16,\ N_3 = 16,\ N_4 = 10$ã€‚

#### å¯å˜å‚æ•°

ç°åœ¨æˆ‘ä»¬å·²ç»ç¡®å®šä¸‹äº†ç½‘ç»œçš„ç»“æ„ï¼Œä¸‹ä¸€æ­¥ç¡®å®šæ‰€æœ‰çš„å‚æ•°ã€‚

å°†ç¬¬ $l$ å±‚çš„ç¬¬ $i$ ä¸ªç»“ç‚¹ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰é™„ä¸Šç‚¹æƒ $a_i^{(l)}$ï¼Œå¹¶ç§°ä¹‹ä¸ºè¯¥ç‚¹çš„ **æ¿€æ´»å€¼**(activation)ã€‚ï¼ˆä¸ç¥ç»ç»†èƒé—´ç”µä¿¡å·å¼ºåº¦ç›¸å¯¹åº”ï¼‰

å°†ç¬¬ $l$ å±‚çš„ç¬¬ $i$ ä¸ªç»“ç‚¹ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰é™„ä¸Šç‚¹æƒ $b_i^{(l)}$ï¼Œå¹¶ç§°ä¹‹ä¸ºè¯¥ç‚¹çš„ **åç½®**(bias)ã€‚ï¼ˆä¸ç¥ç»ç»†èƒäº§ç”Ÿç”µä¿¡å·çš„é˜ˆå€¼ç›¸å¯¹åº”ï¼‰

å°†è¿æ¥ç¬¬ $l-1$ å±‚çš„ç¬¬ $j$ ä¸ªç»“ç‚¹å’Œç¬¬ $l$ å±‚çš„ç¬¬ $i$ ä¸ªç»“ç‚¹çš„è¾¹é™„ä¸Šè¾¹æƒ $w_{ij}^{(l)}$ï¼Œå¹¶ç§°ä¹‹ä¸ºè¯¥è¾¹çš„ **æƒé‡**ã€‚ï¼ˆä¸ä¸¤ä¸ªç¥ç»ç»†èƒé—´è¿æ¥çš„å¼ºå¼±ç›¸å¯¹åº”ï¼Œæƒå€¼è¶Šå¤§ä¿ƒè¿›è¶Šå¼ºï¼Œæƒå€¼è¶Šå°æŠ‘åˆ¶è¶Šå¼ºï¼‰

è¿™ä¸‰ä¸ªå‚æ•°ä¸­åªæœ‰ **åç½®** å’Œ **æƒé‡** è¿™ä¸¤ä¸ªæ˜¯æˆ‘ä»¬éœ€è¦è°ƒå‚çš„ï¼ˆä¹Ÿå°±æ˜¯ç¥ç»ç½‘ç»œéœ€è¦è°ƒæ•´çš„å‚æ•°ï¼‰ï¼Œ**æ¿€æ´»å€¼** æ˜¯å¯ä»¥æ ¹æ®è¾“å…¥æ•°æ®é€’æ¨å‡ºæ¥çš„ï¼ˆåæ–‡ä¼šç»†è®²ï¼‰ã€‚

ç°åœ¨ç®€å•è®¡ç®—ä¸€ä¸‹ï¼Œä¸€å…±è¦è°ƒå¤šå°‘ä¸ªå‚æ•°ï¼š

$$
\sum_{l=2}^{L}N_{l-1}\cdot N_l + N_l = \sum_{l=2}^{L}(N_{l-1}+1)\cdot N_l
$$

å¯¹äºä¸Šé¢è®¾è®¡çš„è¯†åˆ«æ‰‹å†™æ•°å­—çš„ç½‘ç»œä¸ºä¾‹ï¼Œéœ€è¦è°ƒæ•´çš„å‚æ•°ä¸ªæ•°ä¸ºï¼š

$$
(784+1)\cdot 16+(16+1)\cdot 16+(16+1)\cdot 10 = 13002
$$

> è¿™ä¸ªå‚æ•°ä¸ªæ•°ç¡®å®æœ‰ç‚¹å“ˆäººğŸ˜¢ï¼Œè¿™ä¹Ÿå°±æ˜¯è®¡ç®—å­¦ä¹ æ—¶é—´é•¿çš„åŸå› ï¼›**æ¢¯åº¦ä¸‹é™æ³•** å°±æ˜¯ä¸€ç§è°ƒæ•´è¿™ $13002$ ä¸ªå‚æ•°çš„ä¸€ä¸ªç®—æ³•ï¼ˆä¸‹æ–‡ä¼šè¯¦ç»†è§£é‡Šï¼‰ã€‚

![ä¸¾ä¸ªä¾‹å­](https://s4.ax1x.com/2022/01/29/HSQEHe.png)

> ä»”ç»†çœ‹ï¼Œå¯èƒ½å‘ç° $w_{ij}^{(l)}$ çš„ä¸‹æ ‡ $ij$ æ˜¯ä¸æ˜¯å†™åäº†ï¼ˆåº”è¯¥ä»å·¦åˆ°å³ç„¶ï¼Œè€Œæ˜¯ä»å³åˆ°å·¦ï¼‰ï¼Œå…¶å®ä¸ç„¶ï¼Œæˆ‘ä»¬æŠŠä¸Šè¿°å˜é‡ï¼Œä»¥æ¯ä¸€å±‚ä¸ºå•ä½ï¼Œä»¥çŸ©é˜µçš„å½¢å¼å†™å‡ºæ¥ï¼Œè¿™æ ·ä¸‹æ ‡å°±å’ŒçŸ©é˜µå¯¹åº”ä¸Šäº†ã€‚

$$
a^{(l)} = \left[\begin{matrix}
\ a_1^{(l)}&\cdots&a_{N_l}^{(l)}\ 
\end{matrix}\right]^{T}
$$
$$
b^{(l)} = \left[\begin{matrix}
\ b_1^{(l)}&\cdots&b_{N_l}^{(l)}\ 
\end{matrix}\right]^{T}
$$
$$
w^{(l)} = \left[\begin{matrix}
w_{11}^{(l)}&w_{12}^{(l)}&\cdots&w_{1,N_{l-1}}^{(l)}\\
w_{21}^{(l)}&w_{22}^{(l)}&\cdots&w_{2,N_{l-1}}^{(l)}\\
\vdots&\vdots&\ddots&\vdots\\
w_{N_l,1}^{(l)}&w_{N_l,2}^{(l)}&\cdots&w_{N_l,N_{l-1}}^{(l)}\\
\end{matrix}\right]_{(N_l\times N_{l-1})}
$$

å†™æˆçŸ©é˜µçš„å½¢å¼å½“ç„¶æ˜¯ä¸ºäº†æ–¹ä¾¿åç»­è½¬ç§»æ“ä½œäº†~

$$

$$

#### æ¿€æ´»å‡½æ•°

æ¥ä¸‹æ¥è¦å¼•å…¥ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¿€æ´»å€¼ï¼Œæ¯”è¾ƒå¸¸ç”¨çš„å‡½æ•°æ˜¯ $\test{Sigmoid}$ å‡½æ•°ï¼Œå³ $\test{S}$ å‹å‡½æ•°ã€‚

$$
f(x) = \frac{1}{1+e^{-x}}
$$

![Sigmoidå‡½æ•°å›¾åƒ](https://s4.ax1x.com/2022/01/29/HSa8tf.png)

è€Œç°åœ¨ç”¨çš„æ›´å¤šçš„åº”è¯¥æ˜¯ $\text{ReLU}$ çº¿æ€§æ•´æµå‡½æ•°ï¼ˆRectified Linear Unitï¼‰

$$
\text{ReLU}(x) = \max(0,a)
$$

![ReLUå‡½æ•°å›¾åƒ](https://s4.ax1x.com/2022/01/29/HSdPgg.png)

ä½†åœ¨ä¸‹é¢è¯†åˆ«æ‰‹å†™æ•°å­—åº”ç”¨ä¸­æˆ‘è¿˜æ˜¯ç”¨çš„æ˜¯ $\text{Sigmoid}$ å‡½æ•°~

å®šä¹‰ï¼Œå‡½æ•°ä½œç”¨åœ¨çŸ©é˜µä¸Šï¼Œå³ä½œç”¨åœ¨çŸ©é˜µä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Šï¼š

$$
\begin{aligned}
&Let\ A = \left[\begin{matrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nm}
\end{matrix}\right]\\
&Denote\ f(A) := \left[\begin{matrix}
f(a_{11})&\cdots&f(a_{1m})\\
\vdots&\ddots&\vdots\\
f(a_{n1})&\cdots&f(a_{nm})
\end{matrix}\right]
\end{aligned}
$$

#### é€’æ¨è®¡ç®—æ¿€æ´»å€¼

ç”±äºç¬¬ $l$ å±‚çš„æ¯ä¸€ä¸ªæ¿€æ´»å€¼ï¼Œéƒ½ä¸ $l-1$ å±‚çš„æ‰€æœ‰æ¿€æ´»å€¼ç›¸å…³ï¼Œæ‰€ä»¥è®¡ç®— $l$ å±‚çš„ç¬¬ $i$ ä¸ªæ¿€æ´»å€¼ï¼Œå³å°†ç¬¬ $l-1$ çš„çš„æ‰€æœ‰æ¿€æ´»å€¼ä¸å®ƒè¿æ¥çš„è¾¹åšåŠ æƒä¹‹å’Œï¼Œå†åŠ ä¸Šè¯¥ç‚¹çš„åç½®ï¼Œæœ€åå¥—ä¸Šæ¿€æ´»å‡½æ•°å³å¯ï¼Œä¸‹é¢è¿™ä¸ªå›¾å½¢è±¡çš„é˜è¿°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶è¡¨ç°å‡ºäº†å’ŒçŸ©é˜µçš„è”ç³»ã€‚

> ä¸ç¥ç»å…ƒæ˜¯å¦æ¿€å‘ç›¸å¯¹åº”ï¼Œå½“å‰ç¥ç»å…ƒæ¥æ”¶ä¸å®ƒè¿æ¥çš„ç¥ç»å…ƒçš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯è¿›è¡Œæ±‡æ€»åï¼Œå…ˆå‡å»å½“å‰ç¥ç»å…ƒçš„é˜ˆå€¼åï¼Œå†é€šè¿‡ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œæœ€ååˆ¤æ–­è‡ªå·±æ˜¯å¦éœ€è¦æ¿€å‘ã€‚

![æ¿€æ´»å€¼è®¡ç®—æ–¹æ³•](https://s4.ax1x.com/2022/01/29/HSdfxg.png)

æ ¹æ®ä¸Šè¿°å®šä¹‰ï¼Œä¸éš¾å†™å‡ºæ¯åé¢ $L-1$ å±‚çš„æ¿€æ´»å€¼çš„é€’æ¨å¼

$$
a^{(l)} = f(w^{(l)}a^{(l-1)}+b^{(l)})\quad (2\leqslant l\leqslant L)
$$

#### è®¡ç®—å•ä¸ªæ ·æœ¬çš„ä»£ä»·ï¼ˆè¯¯å·®ï¼‰

ç°åœ¨æˆ‘ä»¬å·²ç»å¯ä»¥é€šè¿‡ç¬¬ä¸€å±‚çš„æ¿€æ´»å€¼ï¼ˆè¾“å…¥å±‚ï¼‰ï¼Œè®¡ç®—åé¢æ¯ä¸€å±‚çš„æ¿€æ´»å€¼äº†ï¼Œå½“è®¡ç®—åˆ°æœ€åä¸€å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰æ—¶ï¼Œæ­£å‘é€’æ¨ç»“æŸã€‚

å¯¹äºå½“å‰è¿™ä¸ªç½‘ç»œå’Œç»™å®šæ ·æœ¬ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è¾“å‡ºå±‚æ¿€æ´»å€¼æœ€å¤§çš„ç‚¹çš„ç¼–å·ï¼Œä½œä¸ºè¿™ä¸ªç½‘ç»œå¯¹è¯¥æ ·æœ¬çš„è¾“å‡ºã€‚

é‚£ä¹ˆå¾ˆå¯èƒ½å‡ºç°è¾“å‡ºå±‚æ•°æ®æ··ä¹±ä¸ç›®æ ‡å€¼å®Œå…¨ä¸ç›¸å…³çš„æƒ…å†µï¼Œé‚£ä¹ˆè¿™å°±æ˜¯æ¥ä¸‹æ¥ï¼Œä¹Ÿæ˜¯æœ€é‡è¦çš„éƒ¨åˆ†â€”â€”å‚æ•°ä¼˜åŒ–ï¼Œå³é€šè¿‡è°ƒæ•´ $w, b$ å‚æ•°ï¼Œä»è€Œä½¿å¾—è¾“å‡ºå±‚çš„æ•°æ®ä¸ç›®æ ‡å€¼æ¥è¿‘ï¼Œæœ€ç»ˆå¯¹äºä¸€ä¸ªå®Œå…¨æ²¡è§è¿‡çš„æ ·æœ¬ï¼ŒåŒæ ·ä¹Ÿèƒ½ç»™å‡ºæ­£ç¡®çš„ç»“æœï¼Œè¾¾åˆ°æ·±åº¦å­¦ä¹ çš„è¦æ±‚ã€‚

é‚£ä¹ˆï¼Œå°±å…ˆè¦æè¿°è¾“å‡ºå±‚å’Œç›®æ ‡å€¼çš„è¯¯å·®å¤§å°ï¼Œä»¤ç›®æ ‡å€¼ä¸º

$$
y = \left[\begin{matrix}
\ y_1&\cdots&y_{N_L}\ 
\end{matrix}\right]^{T}
$$

> æ¯”å¦‚ï¼Œä»¥è¯†åˆ«æ•°å­—ä¸ºä¾‹ï¼Œæ¯ä¸€å¼ å›¾ç‰‡å¯¹åº”äº†ä¸€ä¸ªæ•°å­—ï¼Œè¿™ä¸ªæ•°å­—å°±æ˜¯ç›®æ ‡ï¼Œä»¤è¯¥æ•°å­—ä¸º $m$ï¼Œå› ä¸ºè¾“å‡ºå±‚æ˜¯ä»¥æ¿€æ´»å€¼æœ€å¤§çš„ç‚¹ä¸ºè¾“å‡ºç»“æœçš„ï¼Œé‚£ä¹ˆç›®æ ‡å€¼å°±å¯ä»¥è®¾ç½®ä¸º 
$$
\begin{aligned}
y =&\ \left[\begin{matrix}\ 0&\cdots&0&1&0&\cdots&0\ \end{matrix}\right]^{T}& \text{ç¬¬}m\text{ç»´ä¸º}1,\text{å…¶ä»–ä¸º}0\\
=&\ \left[a_{ij}\right]_{(N_L\times 1)},&\text{å…¶ä¸­}a_{ij} = \delta_{im}
\end{aligned}
$$

å…¶ä¸­
$$
\delta_{ij} = \begin{cases}
1,&i=j\\
0,&i\neq j
\end{cases}
$$

æ¥ä¸‹æ¥å®šä¹‰è¾“å‡ºå±‚çš„æ¿€æ´»å€¼å’Œç›®æ ‡å€¼çš„è¯¯å·®å¤§å°ï¼Œè¿™é‡Œå°†å…¶ç§°ä¸º **ä»£ä»·**ï¼ˆCostï¼‰ï¼ˆä¹Ÿæœ‰å«åšLossçš„ï¼Œâ€œæŸå¤±/è¯¯å·®â€ï¼‰ï¼Œæ–¹æ³•å°±æ˜¯ç®€å•çš„æ–¹å·®å½¢å¼ï¼ˆç”±äºä¸æ˜¯çº¿æ€§å½¢å¼ï¼Œæ— æ³•è¡¨ç¤ºæˆçŸ©é˜µå½¢å¼ï¼‰ï¼š

$$
C = \sum_{i=1}^{N_L}(a_{i}^{(L)}-y_i)^2
$$

#### æ¢¯åº¦ä¸‹é™æ³•

> è¿™ä¸ªä¹Ÿå°±æ˜¯æ•´ä¸ªBPç¥ç»ç½‘ç»œç®—æ³•æœ€æ ¸å¿ƒçš„éƒ¨åˆ†

è€ƒè™‘å¦‚ä½•å°† $C$ å‡å°ã€‚ç”±äºæ¯ä¸ªå‚æ•° $w,b$ å¯¹ $C$ çš„å½±å“å„ä¸ç›¸åŒï¼ˆæœ‰çš„ $w$ å˜åŒ–ä¸€ç‚¹ï¼Œ$C$ å˜åŒ–éå¸¸å¤§ï¼Œè€Œæœ‰çš„åˆ™åä¹‹ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ªå‚æ•°å˜åŒ–çš„å¤§å°è‚¯å®šæ˜¯ä¸åŒçš„ï¼‰ï¼Œè€Œä¸”å¦‚æœåªé’ˆå¯¹ä¸€ä¸ªæ ·æœ¬ï¼Œå‚æ•°å˜åŒ–è¿‡å¤§ï¼Œå¯èƒ½å¯¹å¦ä¸€ä¸ªæ ·æœ¬è®¡ç®—ç»“æœï¼Œå‘ç°å˜å¾—æ›´å·®äº†ã€‚é‚£ä¹ˆå¦‚ä½•å¯»æ‰¾ä¸€ä¸ªå¹³è¡¡å‚æ•°ä½¿å¾—æ¯ä¸ªæ ·æœ¬çš„ä»£ä»·éƒ½æ‰“åˆ°æœ€å°ï¼Œå°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•è§£å†³é—®é¢˜ã€‚

æˆ‘ä»¬å¯ä»¥å°† $C$ è§†ä¸ºä¸€ä¸ªå› å˜é‡ï¼Œ $w, b$ ä½œä¸ºè‡ªå˜é‡ï¼š

$$
C = C(w_{11}^{(2)},w_{12}^{(2)},\cdots,b_1^{(2)}\cdots,w_{11}^{(L)},w_{12}^{(L)},\cdots,b_1^{(L)},\cdots)
$$

é‚£ä¹ˆ $C$ å°±å¯ä»¥è§†ä¸ºä¸€ä¸ª $n$ ç»´å‡½æ•°ï¼Œç”±äºä¸­é—´çš„å…³ç³»éƒ½æ˜¯çº¿æ€§çš„ï¼Œ$\text{Sigmoid}$ å‡½æ•°ä¹Ÿæ˜¯è¿ç»­çš„ï¼ˆ$\text{ReLU}$ å‡½æ•°åˆ†æ®µè¿ç»­ï¼‰ï¼Œæ‰€ä»¥ $C$ æ˜¯ä¸€ä¸ªè¿ç»­å¯å¾®å‡½æ•°ï¼Œæˆ‘ä»¬è€ƒè™‘å¦‚ä½•å˜åŒ–è¿™äº›è‡ªå˜é‡ï¼Œä½¿å¾—å› å˜é‡ $C$ å‡å°åœ°æœ€å¿«ã€‚

ç”±äºå‡½æ•°åœ¨å¯¼æ•°æ–¹å‘ä¸Šå¢å¤§é€Ÿåº¦æœ€å¿«ï¼Œ$n$ ç»´å‡½æ•°åœ¨æ¢¯åº¦çš„æ–¹å‘ä¸Šå¢å¤§é€Ÿåº¦æœ€å¿«ï¼Œäºæ˜¯è€ƒè™‘ $C$ çš„æ¢¯åº¦ï¼š

$$
\nabla C = \left[\begin{matrix}
\ \dfrac{\partial C}{\partial w_{11}^{(2)}}&\dfrac{\partial C}{\partial w_{12}^{(2)}}&\cdots&\dfrac{\partial C}{b_1^{(2)}}\cdots&\dfrac{\partial C}{w_{11}^{(L)}}&\dfrac{\partial C}{w_{12}^{(L)}}&\cdots&\dfrac{\partial C}{\partial b_1^{(L)}}&\cdots\ 
\end{matrix}\right]^{T}
$$

ç”±äºæˆ‘ä»¬è¦ä½¿å¾— $C$ å‡å°é€Ÿåº¦æœ€å¿«ï¼Œæ‰€ä»¥æ¢¯åº¦çš„åæ–¹å‘å°±æ˜¯æ¯ä¸€ä¸ªè‡ªå˜é‡çš„å˜åŒ–æ–¹å‘ï¼Œäºæ˜¯ç°åœ¨é—®é¢˜è½¬åŒ–ä¸ºæ±‚è§£ $\nabla C$ï¼Œè¿™æ˜¯ä¸€ä¸ªé€’æ¨çš„è¿‡ç¨‹ï¼š

ä¸ºæ–¹ä¾¿ä¹¦å†™åšå‡ºå¦‚ä¸‹çš„å®šä¹‰

> ä»¥ä¸‹çš„å®šä¹‰å’Œæ¨å¯¼å‡æ˜¯æˆ‘è‡ªå·±è®¡ç®—çš„ï¼Œä¸èƒ½ä¿è¯æ­£ç¡®æ€§ï¼Œå»ºè®®è‡ªå·±å°è¯•å…ˆæ¨ä¸€éï¼ˆä½†æœ€åç¼–ç å®ç°åï¼Œå‘ç°å­¦ä¹ æ•ˆæœä¸é”™ğŸ˜‰ï¼‰

ä»¤ $z^{(l)} = w^{(l)}a^{(l-1)}+b^{(l)}$ï¼Œåˆ™ $a^{(l)} = f(z^{(l)})$ã€‚

ä»¤ $f:\mathbb R^n\rightarrow \mathbb R$ ä¸ºè¿ç»­å¯å¯¼å‡½æ•°ï¼Œ$A$ çš„å…ƒç´ å‡ä¸º $f$ çš„å˜é‡ï¼Œå®šä¹‰åå¯¼æ•°ä½œç”¨åœ¨çŸ©é˜µä¸Šï¼Œå³ä½œç”¨åœ¨çŸ©é˜µä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Šï¼Œè¿˜å®šä¹‰äº†ä¸¤ä¸ªåŒé˜¶çŸ©é˜µä¹‹é—´çš„â€œç‚¹ä¹˜â€ $\odot$ å…³ç³»ï¼Œå³å¯¹åº”ä½ä¸Šçš„å…ƒç´ ç›¸ä¹˜ï¼ˆè¿™äº›ç¬¦å·ä¸‹é¢éƒ½ä¼šç”¨åˆ°ï¼Œç”¨äºç®€åŒ–è¿ç®—ï¼‰ï¼š

$$
\begin{aligned}
&Let\ A = \left[\begin{matrix}
a_{11}&\cdots&a_{1m}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nm}
\end{matrix}\right], \ 
B = \left[\begin{matrix}
b_{11}&\cdots&b_{1m}\\
\vdots&\ddots&\vdots\\
b_{n1}&\cdots&a_{nm}
\end{matrix}\right]\\
&Denote\ \frac{\partial f}{\partial A} := \left[\begin{matrix}
\frac{\partial f}{\partial a_{11}}&\cdots&\frac{\partial f}{\partial a_{1m}}\\
\vdots&\ddots&\vdots\\
\frac{\partial f}{\partial a_{n1}}&\cdots&\frac{\partial f}{\partial a_{nm}}
\end{matrix}\right]\\
& Denote\ A\odot B := \left[\begin{matrix}
a_{11}b_{11}&\cdots&a_{1m}b_{1m}\\
\vdots&\ddots&\vdots\\
a_{n1}b_{n1}&\cdots&a_{nm}b_{nm}
\end{matrix}\right]
\end{aligned}
$$

å…ˆæ•´ç†ä¸€ä¸‹æ‰€æœ‰çš„å¼å­ï¼ˆéçŸ©é˜µå½¢å¼ï¼Œä¸ºäº†æ±‚åå¯¼ï¼‰ï¼š

$$
\begin{cases}
\displaystyle C = \sum_{i=1}^{N_L}(a_{i}^{(L)}-y_i)^2\\
\displaystyle z_i^{(l)} = \sum_{j=1}^{N_{l-1}}w_{ij}^{(l)}a_j^{(l-1)}+b_i^{(l)}\\
\displaystyle a_i^{(l)} = f(z_i^{(l)})
\end{cases}
$$

ä¸‹é¢å¼€å§‹æ±‚åå¯¼ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ **é“¾å¼æ³•åˆ™**

å…ˆæ¨ç¬¬ $L$ å±‚çš„åå¯¼

$$
\frac{\partial C}{\partial a_i^{(L)}} = 2(a_i^{(L)} - y_i)
$$

$$
\begin{aligned}
\frac{\partial C}{\partial b_i^{(L)}} =&\  \frac{\partial C}{\partial a_i^{(L)}}\cdot\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}}\cdot\frac{\partial z_i^{(L)}}{\partial b_i^{(L)}}\\
=&\ 2(a_i^{(L)}-y_i)\cdot f'(z_i^{(L)})\cdot 1
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial C}{\partial w_{ij}^{(L)}} =&\  \frac{\partial C}{\partial a_i^{(L)}}\cdot\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}}\cdot\frac{\partial z_i^{(L)}}{\partial w_{ij}^{(L)}}\\
=&\ 2(a_i^{(L)}-y_i)\cdot f'(z_i^{(L)})\cdot a_j^{(L-1)}
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial C}{\partial a_j^{(L-1)}} =&\ \sum_{i=1}^{N_L} \frac{\partial C}{\partial a_i^{(L)}}\cdot\frac{\partial a_i^{(L)}}{\partial z_i^{(L)}}\cdot\frac{\partial z_i^{(L)}}{\partial a_j^{(L-1)}}\\
=&\ \sum_{i=1}^{N_L}2(a_i^{(L)}-y_i)\cdot f'(z_i^{(L)})\cdot w_{ij}^{(L)}
\end{aligned}
$$

ç­‰ä»·çš„çŸ©é˜µå½¢å¼

$$
\begin{aligned}
\frac{\partial C}{\partial b^{(L)}} =&\ 2(a^{(L)}-y)\odot f'(z^{(L)})\\
\frac{\partial C}{\partial w^{(L)}} =&\ \left(2(a^{(L)}-y)\odot f'(z^{(L)})\right)\cdot\left(a^{(L-1)}\right)^T\\
\frac{\partial C}{\partial a^{(L-1)}} =& \left(w^{(L)}\right)^T\cdot\left(2(a^{(L)}-y)\odot f'(z^{(L)})\right)
\end{aligned}
$$

ç»è¿‡è§‚å¯Ÿå‘ç°ï¼Œåé¢ä¸¤ä¸ªå¼å­ä¸­åŒ…å«äº†ç¬¬ä¸€ä¸ªå¼å­ï¼ˆè¿˜æ˜¯å¾ˆæœ‰æ„æ€çš„ï¼‰ï¼Œäºæ˜¯

$$
\begin{aligned}
\frac{\partial C}{\partial b^{(L)}} =&\ 2(a^{(L)}-y)\odot f'(z^{(L)})\\
\frac{\partial C}{\partial w^{(L)}} =&\ \frac{\partial C}{\partial b^{(L)}}\cdot\left(a^{(L-1)}\right)^T\\
\frac{\partial C}{\partial a^{(L-1)}} =& \left(w^{(L)}\right)^T\cdot\frac{\partial C}{\partial b^{(L)}}
\end{aligned}
$$

ç±»ä¼¼çš„ï¼Œæœ‰äº† $\displaystyle\frac{\partial C}{\partial a^{(L-1)}}$ï¼Œåé¢çš„é¡¹å¯ä»¥ç±»æ¯”æ¨å‡ºï¼Œäºæ˜¯æ•´ä¸ªåå¯¼çš„é€’æ¨å¼å¦‚ä¸‹ï¼š

$$
\begin{aligned}
\frac{\partial C}{\partial b^{(l)}} =&\ \frac{\partial C}{\partial a^{(l)}}\odot f'(z^{(l)})\\
\frac{\partial C}{\partial w^{(l)}} =&\ \frac{\partial C}{\partial b^{(l)}}\cdot\left(a^{(l-1)}\right)^T\\
\frac{\partial C}{\partial a^{(l-1)}} =& \left(w^{(l)}\right)^T\cdot\frac{\partial C}{\partial b^{(l)}}
\end{aligned}\quad (2\leqslant l\leqslant L)
$$

é€’æ¨åˆå§‹å€¼ï¼š$\displaystyle\frac{\partial C}{\partial a^{(L)}} = 2\left(a^{(L)}-y\right)$

äºæ˜¯å°±å¯ä»¥å¿«ä¹çš„é€’æ¨äº†ğŸ‰ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ç»“æŸï¼‰

#### éšæœºåˆ†ç»„æ›´æ–°

æœ‰äº†æ¢¯åº¦å€¼ï¼Œå°±çŸ¥é“æ¯ä¸€ä¸ªæ ·æœ¬å¯¹æ¯ä¸€ä¸ªå‚æ•°è°ƒæ•´çš„å¤§å°äº†ï¼Œä½†æ˜¯å¦‚æœå¯¹æ¯ä¸€ä¸ªæ ·æœ¬éƒ½è¿›è¡Œä¸€æ¬¡è°ƒå‚ï¼Œæ¬¡æ•°è¿‡å¤šï¼Œè€Œä¸”å¯èƒ½å¯¼è‡´å‚æ•°è¿…é€Ÿä¸‹é™åˆ°æŸä¸€ä¸ªå€¼ä¸Šï¼Œå¯¼è‡´æ— æ³•è·å¾—å…¨å±€æœ€ä¼˜å€¼ã€‚

è€ƒè™‘èƒ½å¦å°†æ ·æœ¬éšæœºåˆ†ä¸ºä¸€ç»„ä¸€ç»„çš„ï¼Œæ¯ä¸€ç»„ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œè®¡ç®—ä¸€æ¬¡æ¢¯åº¦çš„å¹³å‡å€¼ï¼Œæœ€åå†å¯¹ç½‘ç»œå‚æ•°è¿›è¡Œä¸€æ¬¡ä¿®æ”¹ï¼Œè¿™å°±æ˜¯éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„æ€è·¯ã€‚

å…ˆå°†æ ·æœ¬æ‰“ä¹±ï¼Œè®¾å®šä¸€ä¸ª $\text{Mini-batch}$ å¤§å°ï¼Œä»¥ä¸€ä¸ª $\text{Mini-batch}$ ä½œä¸ºæ¢¯åº¦ä¸‹é™çš„ä¸€æ­¥ï¼Œå¯¹ç½‘ç»œå‚æ•°è¿›è¡Œä¿®æ”¹ï¼Œå¦‚æ­¤åå¤è¿›è¡Œè¿­ä»£ï¼Œä»è€Œä½¿å¾—ä»£ä»·å‡½æ•°æ”¶æ•›åˆ°ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼ä¸Šã€‚

#### åˆå§‹åŒ–æ•°æ®

æœ€åˆç½‘ç»œçš„å»ºç«‹æ˜¯æ²¡æœ‰ä»»ä½•å‚æ•°çš„ï¼Œæ‰€ä»¥éƒ½æ˜¯ $w,b$ éƒ½æ˜¯éšæœºäº§ç”Ÿçš„ï¼Œç”±äº $\text{Sigmoid}$ å‡½æ•°èƒ½å¯¹æ•°å€¼é™åˆ¶åˆ° $(0,1)$ ä¹‹é—´ï¼Œæ‰€ä»¥éšæœºç»™åˆå§‹å€¼ç†è®ºä¸Šæ˜¯å¯ä»¥çš„ã€‚

### ç¥ç»ç½‘ç»œC++å®ç°

> ç†è®ºæˆç«‹ï¼Œå¼€å§‹å®ç°

æˆ‘å–œæ¬¢æŠŠå®Œæˆä¸€æ•´ä¸ª **æ¢¯åº¦ä¸‹é™æ³•** çš„è¿‡ç¨‹ç§°ä¸ºä¸€æ¬¡ **å­¦ä¹ **ğŸ˜œï¼ŒæŠŠå½“å‰çš„ç½‘ç»œå‚æ•°ï¼ˆä¸»è¦æ˜¯ $b,w$ çš„å€¼ï¼‰ç§°ä¸ºå½“å‰çš„ **å­¦ä¹ æˆæœ**ã€‚

#### æ ·ä¾‹æ•°æ®çš„å¤„ç†

è¿™é‡Œä»¥æ‰‹å†™æ•°å­—è¯†åˆ«ä¸ºä¾‹å­ï¼Œç½‘ä¸Šå·²æœ‰ç°æˆçš„æ•°æ®é›†ç”¨äºè®­ç»ƒï¼Œå¦‚ [Minist](http://yann.lecun.com/exdb/mnist/) å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ•°æ®é›†ï¼Œé‡Œé¢åŒ…å«äº† $60000$ ä¸ªè®­ç»ƒæ•°æ®ï¼Œ $10000$ ä¸ªæµ‹è¯•æ•°æ®ã€‚

ä½†ç”±äºè¿™äº›æ•°æ®éƒ½æ˜¯ Python ä¸‹çš„ï¼Œä¸ºäº†ä½¿ç”¨ C++ å¤„ç†ï¼Œæ‰€ä»¥éœ€è¦å…ˆç”¨ Python è¿›è¡Œè§£åŒ…ï¼Œ[è§£åŒ…æ•™ç¨‹](https://blog.csdn.net/simple_the_best/article/details/75267863)ã€‚

æˆ‘æ˜¯å…ˆæŠŠè®­ç»ƒæ•°æ®å¯¼å‡ºä¸ºä¸¤ä¸ªæ–‡ä»¶ï¼Œ`train.in` é‡Œé¢æ˜¯ä¸€ä¸ª $60000\times 784$ åƒç´ çŸ©é˜µï¼ŒçŸ©é˜µæ¯ä¸€è¡Œæ˜¯ç”±åƒç´  $28\times 28$ çš„å›¾ç‰‡æ‹‰ä¼¸æˆçš„ï¼ŒçŸ©é˜µä¸­æ¯ä¸€ä¸ªå…ƒç´ åœ¨ $[0,255]$ ä¹‹é—´ï¼Œè¡¨ç¤ºç°åº¦å€¼ï¼Œ`train.out` æ˜¯ä¸€ä¸ª $60000\times 1$ çš„ç­”æ¡ˆå‘é‡ï¼Œå¯¹åº” `train.in` æ¯ä¸€è¡Œçš„æ‰€å¯¹åº”çš„å›¾åƒçš„æ•°å­—ã€‚

`test.in` æ˜¯ä¸€ä¸ª $10000\times 784$ åƒç´ çŸ©é˜µï¼Œæ•°æ®èŒƒå›´åŒ `train.in`ï¼Œ`test.out` æ˜¯ä¸€ä¸ª $10000\times 1$ çš„ç­”æ¡ˆå‘é‡ï¼Œå«ä¹‰åŒ `train.out`ï¼Œè¿™ä¸ªæ•°æ®é›†ç”¨äºå¯¹è®­ç»ƒçš„ç½‘ç»œè¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºè¿™äº›æ•°æ®åœ¨ `train.in` ä¸­æ²¡æœ‰å‡ºç°è¿‡ï¼Œå¯ä»¥é€šè¿‡å¯¹è¯¥æ•°æ®é›†çš„æµ‹è¯•åˆ¤æ–­ç½‘ç»œçš„è¯†åˆ«æ•ˆæœã€‚

#### å­¦ä¹ æˆæœçš„ä¿å­˜

ç”±äºæ¯ä¸€æ­¥çš„å­¦ä¹ æˆæœï¼Œæ˜¯å¯ä»¥ä½œä¸ºä¸‹æ¬¡å­¦ä¹ å¼€å§‹çš„æ•°æ®çš„ï¼Œæ‰€ä»¥éœ€è¦åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¿å­˜ä¸‹æ¥ï¼Œè¿™é‡Œç»™å‡ºæˆ‘è®¾è®¡çš„ä¸€ä¸ªä¿å­˜æ ¼å¼

```c++
L // ç½‘ç»œå±‚æ•°
N[0] N[1] ... N[L-1] // æ¯ä¸€å±‚çš„ç»“ç‚¹ä¸ªæ•°
net[1].w // ç½‘ç»œç¬¬1å±‚çš„wçŸ©é˜µ(N[1]*N[0])
net[1].b // ç½‘ç»œç¬¬1å±‚çš„bçŸ©é˜µ(N[1]*1)
...
net[L-1].w // ç½‘ç»œæœ€åä¸€å±‚çš„wçŸ©é˜µ(N[L-1]*N[L-2])
net[L-1].b // ç½‘ç»œæœ€åä¸€å±‚çš„bçŸ©é˜µ(N[L-1]*1)
```

ç”±äºä»£ç ä¸­çš„ä¸‹æ ‡éƒ½æ˜¯ä» $0$ å¼€å§‹çš„ï¼Œæ‰€ä»¥ `N[i]` å¯¹åº”ä¸Šæ–‡ä¸­çš„å¸¸é‡ $N_{i+1}$ï¼Œ`net[i].w` å¯¹åº”ä¸Šæ–‡ä¸­çš„çŸ©é˜µ $w^{(i+1)}$ï¼Œ`net[i].b` å¯¹åº”ä¸Šæ–‡ä¸­çš„çŸ©é˜µ $b^{(i+1)}$ã€‚

#### çŸ©é˜µå®ç°

çŸ©é˜µéœ€è¦æ»¡è¶³çŸ©é˜µä¹˜æ³•ã€çŸ©é˜µåŠ æ³•ã€çŸ©é˜µä¹˜å¸¸æ•°ã€çŸ©é˜µä¹‹é—´çš„ç‚¹ä¹˜ã€çŸ©é˜µçš„è½¬ç½®ã€çŸ©é˜µçš„è¾“å‡ºï¼Œè¿™å…­ä¸ªæ“ä½œï¼Œä»¥ç»“æ„ä½“æ–¹å¼å®Œæˆï¼Œä»£ç å¦‚ä¸‹ï¼š

```c++
struct mat{ // Matrix Data Struct
	int n, m; // Size of Matrix : n * m
	vdd M;
	mat() {}
	mat(int n, int m, int num = 0) : n(n), m(m) { M = vdd(n, vd(m, num)); }
	mat operator * (const mat &y) const & { // multiply of Matrix
		assert(m == y.n);
		mat z(n, y.m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < y.m; j++)
				for (int k = 0; k < m; k++)
					z.M[i][j] += M[i][k] * y.M[k][j];
		return z;
	}
	mat operator + (const mat &y) const & { // addition of Matrix
		assert(n == y.n && m == y.m);
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] + y.M[i][j];
		return z;
	}
	mat operator * (const double &y) const & { // multiply Matrix and Const
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] * y;
		return z;
	}
	mat dot(mat &x, mat &y) { // dot multiplay of Matrix
		assert(x.n == y.n && x.m == y.m);
		int n = x.n, m = x.m;
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = x.M[i][j] * y.M[i][j];
		return z;
	}
	mat operator ~ () const & { // transpose the Matrix
		mat z(m, n);
		for (int i = 0; i < m; i++)
			for (int j = 0; j < n; j++)
				z.M[i][j] = M[j][i];
		return z;
	}
	void print() { // print the Matrix
		for (int i = 0; i < n; i++) {
			for (int j = 0; j < m; j++) {
				printf("%.2lf ", M[i][j]);
			}
			putchar('\n');
		}
		putchar('\n');
	}
}MAT;
```

#### ä»£ç å®ç°

æ‰€æœ‰çš„ä»£ç å’Œæµ‹è¯•æ•°æ®å‡ä¸Šä¼ è‡³ [Github - ANN---Writing-Number](https://github.com/wty-yy/ANN---Writing-Number)

ä»£ç æœ‰â€œä¸€ç‚¹â€é•¿ï¼ˆå¾ˆè€ƒéªŒè€å¿ƒå’Œå‡†ç¡®æ€§ï¼Œå°±å½“åœ¨åšä¸€é“å¤§å‹OIæ¨¡æ‹Ÿé¢˜äº†ğŸ¤£ï¼‰ï¼Œä¸»è¦æ˜¯çŸ©é˜µçš„å®ç°å’Œè¾“å…¥è¾“å‡ºéƒ¨åˆ†æ¯”è¾ƒå¤æ‚ï¼Œæ ¸å¿ƒéƒ¨åˆ†åªæœ‰50è¡Œå·¦å³ã€‚

å¯¹ä»£ç ä¸­çš„å¸¸é‡è¿›è¡Œä¸‹è§£é‡Šï¼Œè¿™æ ·ä»¥åå°±åªç”¨ä¿®æ”¹è¿™äº›å€¼å°±å¯ä»¥ç”¨äºå…¶ä»–åŠŸèƒ½äº†ğŸ™Œï¼š

```c++
const int T = 60000; // æ€»çš„æ ·ä¾‹æ•°ç›®
const int L = 4; // ç½‘ç»œçš„å±‚æ•°
const int IN = 784; // è¾“å…¥å±‚çš„ç»“ç‚¹ä¸ªæ•°ï¼Œå³N[0]
const int OUT = 10; // è¾“å‡ºå±‚çš„ç»“ç‚¹ä¸ªæ•°ï¼Œå³N[L-1]
const int N[L] = {IN, 16, 16, OUT}; // æ¯ä¸€å±‚çš„ç»“ç‚¹ä¸ªæ•°
db image[T][IN]; // å›¾åƒæ•°æ®ï¼ˆè¾“å…¥æ•°æ®ï¼‰
int ans[T]; // å›¾åƒå¯¹åº”çš„æ•°å­—ï¼ˆç­”æ¡ˆæ•°æ®ï¼‰
const int GROUP = 100; // å­¦ä¹ å°ç»„çš„å¤§å°ï¼ˆæ¯ä¸ª Mini-batch çš„å¤§å°ï¼‰
const int NUM = 600; // å­¦ä¹ å°ç»„çš„ä¸ªæ•°ï¼ˆè¿™é‡Œè¦æ±‚ NUM * GROUP = Tï¼‰
const int TOT = 1500; // å¯¹æ‰€æœ‰æ ·ä¾‹è¿›è¡Œè®­ç»ƒçš„æ¬¡æ•°ï¼ˆè¿™é‡Œå¤šçº¿ç¨‹ä¸‹å¤§æ¦‚ç®—ä¸€æ¬¡60000ä¸ªæ ·ä¾‹ï¼Œéœ€è¦1.5sï¼‰
const int THR = 20; // çº¿ç¨‹æ•°ç›®
```

ä»£ç ä¸­æœ‰å¾ˆå¤šè‹±æ–‡æ³¨é‡Šï¼ŒåŠ©äºç†è§£ã€‚

è¿™æ˜¯ç¬¬ä¸€ä»½ä»£ç `ANN.cpp`ï¼Œåªèƒ½æ”¯æŒå•çº¿ç¨‹å­¦ä¹ 

```c++
#include <bits/stdc++.h>
#define db double
#define ll long long
#define vi vector<int>
#define vii vector<vi >
#define vd vector<db>
#define vdd vector<vd >
#define pii pair<int, int>
#define pdd pair<db, db>
#define vpd vector<pdd >
#define vipd vector<vpd >
#define vp vector<pii >
#define vip vector<vp >
#define mkp make_pair
#define pb push_back
using namespace std;
const int INF = 0x3f3f3f3f;
const int T = 60000; // Number of Total training Data
const int L = 4; // Number of Layers (contains Input layer and Output layer)
const int IN = 784; // Number of Nodes in Layer 1 (Input Layer)
const int OUT = 10; // Number of Nodes in Layer L-1 (Output Layer)
const int N[L] = {IN, 16, 16, OUT}; // Number of Nodes in each Layer
//vd N(L); 
db image[T][IN]; // Image Data
int ans[T]; // Label of Image Data (Answer)
const int GROUP = 100; // Learning Group (Upgrade the network by GROUP numbers of Learning Data)
const int NUM = 600; // Number of Learning Group
const int TOT = 1; // Number of ANN
struct mat{ // Matrix Data Struct
	int n, m; // Size of Matrix : n * m
	vdd M;
	mat() {}
	mat(int n, int m, int num = 0) : n(n), m(m) { M = vdd(n, vd(m, num)); }
	mat operator * (const mat &y) const & { // multiply of Matrix
		assert(m == y.n);
		mat z(n, y.m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < y.m; j++)
				for (int k = 0; k < m; k++)
					z.M[i][j] += M[i][k] * y.M[k][j];
		return z;
	}
	mat operator + (const mat &y) const & { // addition of Matrix
		assert(n == y.n && m == y.m);
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] + y.M[i][j];
		return z;
	}
	mat operator * (const double &y) const & { // multiply Matrix and Const
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] * y;
		return z;
	}
	mat dot(mat &x, mat &y) { // dot multiplay of Matrix
		assert(x.n == y.n && x.m == y.m);
		int n = x.n, m = x.m;
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = x.M[i][j] * y.M[i][j];
		return z;
	}
	mat operator ~ () const & { // transpose the Matrix
		mat z(m, n);
		for (int i = 0; i < m; i++)
			for (int j = 0; j < n; j++)
				z.M[i][j] = M[j][i];
		return z;
	}
	void print() { // print the Matrix
		for (int i = 0; i < n; i++) {
			for (int j = 0; j < m; j++) {
				printf("%.2lf ", M[i][j]);
			}
			putchar('\n');
		}
		putchar('\n');
	}
}MAT;
struct layer { // Layer of the Network
	mat a, w, b, z;
	int id;
	layer() {}
	layer(int id) : id(id) {
		a = mat(N[id], 1);
		if (id) {
			w = mat(N[id], N[id-1]);
			b = mat(N[id], 1);
		}
	}
}baseNet[L]; // basic network
db getrand() { return 1.0 * rand() / RAND_MAX; }
void init() { // initialize Training Data
	freopen("train.in", "r", stdin);
	for (int i = 0; i < T; i++) {
		for (int j = 0; j < IN; j++) {
			scanf("%lf", &image[i][j]);
			image[i][j] /= 255;
		}
	}
	fclose(stdin);
	freopen("train.out", "r", stdin);
	for (int i = 0; i < T; i++) scanf("%d", &ans[i]);
	fclose(stdin);
	printf("Reading complete!\n");
	// image Input TEST
	//for (int i = 0; i < 784; i++) {
	//	printf("%d ", (int)(image[0][i] * 255));
	//	if ((i+1) % 28 == 0) {
	//		putchar('\n');
	//	}
	//}
	freopen("diary.out", "w", stdout);
	fclose(stdout);
}
void Save(int num) { // Print Learning Result
	string s = string("Result") + to_string(num) + string(".out");
	freopen(s.c_str(), "w", stdout);
	printf("%d\n", L);
	for (int i = 0; i < L; i++) printf("%d ", N[i]);
	putchar('\n');
	for (int l = 1; l < L; l++) {
		baseNet[l].w.print();
		baseNet[l].b.print();
	}
	fclose(stdout);
}
// Back Propagation (Learning)
mat f(mat &x) { // activate function (sigmoid)
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (1 + exp(-t));
		}
	}
	return z;
}
mat _f(mat &x) { // Derivative of activate function 
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (exp(t) + exp(-t) + 2);
		}
	}
	return z;
}
// Id of Learning Data and Total gradient
db BP(int id, layer grad[]) { // return Cost
	layer net[L];
	mat y(OUT, 1); // Desired result (Answer)
	for (int i = 0; i < L; i++) net[i] = baseNet[i];
	// initialize Input & Desired Data
	for (int i = 0; i < IN; i++) net[0].a.M[i][0] = image[id][i];
	for (int i = 0; i < OUT; i++)
		if (i == ans[id]) y.M[i][0] = 1;
	// Forward
	for (int l = 1; l < L; l++) {
		net[l].z = net[l].w * net[l-1].a + net[l].b;
		net[l].a = f(net[l].z);
	}
	// Backward
	mat dc_da = (net[L-1].a + (y * (-1))) * 2;
	for (int l = L-1; l >= 1; l--) {
		mat _fz = _f(net[l].z);
		mat dc_db = MAT.dot(dc_da, _fz);
		grad[l].b = grad[l].b + dc_db;
		grad[l].w = grad[l].w + (dc_db * (~net[l-1].a));
		dc_da = (~net[l].w) * dc_db;
	}
	// Cost
	db cost = 0;
	for (int i = 0; i < OUT; i++) cost += pow(net[L-1].a.M[i][0] - y.M[i][0], 2);
	return cost;
}
void ANN() { // Artificial Neural Network
	// initialize the struct of Network
	for (int i = 0; i < L; i++) baseNet[i] = layer(i);
	if (freopen("Result.in", "r", stdin) == NULL) { // initialize w and b randomly
		freopen("/dev/tty", "w", stdout);
		printf("Randomly initialization\n");
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++) {
				for (int j = 0; j < N[l-1]; j++)
					baseNet[l].w.M[i][j] = getrand() * 10 - 5;
				baseNet[l].b.M[i][0] = getrand() * 40 - 20;
			}
		}
	} else { // Using last Learning Data
		freopen("/dev/tty", "w", stdout);
		printf("Get Result.in\n");
		int rL;
		scanf("%d", &rL);
		assert(L == rL);
		vi rN(L);
		for (int i = 0; i < L; i++) {
			scanf("%d", &rN[i]);
			assert(rN[i] == N[i]);
		}
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++)
				for (int j = 0; j < N[l-1]; j++)
					scanf("%lf", &baseNet[l].w.M[i][j]);
			for (int i = 0; i < N[l]; i++)
				scanf("%lf", &baseNet[l].b.M[i][0]);
		}
		fclose(stdin);
	}
	vi perm(T);
	for (int i = 0; i < T; i++) perm[i] = i;
	int fg = 0; // id of Save data
	for (int _i = 0; _i < TOT; _i++) {
		random_shuffle(perm.begin(), perm.end());
		for (int i = 0; i < GROUP * NUM; i += GROUP) {
			layer grad[L]; // average gradient of a group
			for (int l = 0; l < L; l++) grad[l] = layer(l);
			db cost = 0; // average cost of a group
			for (int j = i; j < i + GROUP; j++) { // assign Learning tasks
				cost += BP(perm[j], grad);
			}
			for (int i = 1; i < L; i++) { // Upgrade Network
				baseNet[i].w = baseNet[i].w + grad[i].w * (-1.0 / GROUP);
				baseNet[i].b = baseNet[i].b + grad[i].b * (-1.0 / GROUP);
			}
			freopen("diary.out", "a", stdout);
			printf("%lf\n", cost / GROUP);
			fclose(stdout);
			Save(fg);
			fg ^= 1;
		}
		freopen("/dev/tty", "w", stdout);
		printf("complete turn: %d\n", _i+1);
	}
	// TEST
	//for (int i = 0; i < 10; i++) {
	//	layer grad[L]; // average gradient of the group
	//	for (int l = 0; l < L; l++) grad[l] = layer(l);
	//	for (int j = i; j < i + 1; j++) { // assign Learning tasks
	//		db cost = BP(perm[0], grad);
	//		printf("%lf\n", cost);
	//	}
	//	for (int i = 1; i < L; i++) { // Upgrade Network
	//		baseNet[i].w = baseNet[i].w + grad[i].w * (-1.0 / GROUP);
	//		baseNet[i].b = baseNet[i].b + grad[i].b * (-1.0 / GROUP);
	//	}
	//}
}
signed main() {
	srand(time(NULL));
	init();
	clock_t st = clock(), en;
	ANN();
	en = clock();
	freopen("diary.out", "a", stdout);
	printf("Learning time: %lf s\n", 1.0 * (en - st) / CLOCKS_PER_SEC);
	fclose(stdout);
	return 0;
}
```

è¿™æ˜¯ç¬¬äºŒä¸ªç‰ˆæœ¬çš„ä»£ç `ANN_Parallel.cpp`ï¼Œæ”¯æŒå¤šçº¿ç¨‹ï¼ˆæ­¤ä»£ç æ˜¯åœ¨ `Linux` ä¸Šè¿è¡Œçš„ï¼Œå¦‚æœè¦åœ¨ `Windows` ä¸‹è¿è¡Œï¼Œéœ€è¦æ”¯æŒ `thread` è¿™ä¸ªå‡½æ•°ï¼Œæ–¹æ³•å¯ä»¥è§è¿™ç¯‡åšå®¢ [CSDN - mingw-w64å®‰è£…æ”¯æŒc++11ä¸­threadï¼ˆwindowsä¸‹ï¼‰](https://blog.csdn.net/name_z/article/details/43818593)ï¼‰

```c++
#include <bits/stdc++.h>
#define db double
#define ll long long
#define vi vector<int>
#define vii vector<vi >
#define vd vector<db>
#define vdd vector<vd >
#define pii pair<int, int>
#define pdd pair<db, db>
#define vpd vector<pdd >
#define vipd vector<vpd >
#define vp vector<pii >
#define vip vector<vp >
#define mkp make_pair
#define pb push_back
using namespace std;
const int INF = 0x3f3f3f3f;
const int T = 60000; // Number of Total training Data
const int L = 4; // Number of Layers (contains Input layer and Output layer)
const int IN = 784; // Number of Nodes in Layer 1 (Input Layer)
const int OUT = 10; // Number of Nodes in Layer L-1 (Output Layer)
const int N[L] = {IN, 16, 16, OUT}; // Number of Nodes in each Layer
//vd N(L); 
db image[T][IN]; // Image Data
int ans[T]; // Label of Image Data (Answer)
const int GROUP = 100; // Learning Group (Upgrade the network by GROUP numbers of Learning Data)
const int NUM = 600; // Number of Learning Group
const int TOT = 1500; // Number of ANN, 10 24s, 1500 45min, 2000 1h
const int THR = 20; // Number of Threads
struct mat{ // Matrix Data Struct
	int n, m; // Size of Matrix : n * m
	vdd M;
	mat() {}
	mat(int n, int m, int num = 0) : n(n), m(m) { M = vdd(n, vd(m, num)); }
	mat operator * (const mat &y) const & { // multiply of Matrix
		assert(m == y.n);
		mat z(n, y.m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < y.m; j++)
				for (int k = 0; k < m; k++)
					z.M[i][j] += M[i][k] * y.M[k][j];
		return z;
	}
	mat operator + (const mat &y) const & { // addition of Matrix
		assert(n == y.n && m == y.m);
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] + y.M[i][j];
		return z;
	}
	mat operator * (const double &y) const & { // multiply Matrix and Const
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] * y;
		return z;
	}
	mat dot(mat &x, mat &y) { // dot multiplay of Matrix
		assert(x.n == y.n && x.m == y.m);
		int n = x.n, m = x.m;
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = x.M[i][j] * y.M[i][j];
		return z;
	}
	mat operator ~ () const & { // transpose the Matrix
		mat z(m, n);
		for (int i = 0; i < m; i++)
			for (int j = 0; j < n; j++)
				z.M[i][j] = M[j][i];
		return z;
	}
	void print() { // print the Matrix
		for (int i = 0; i < n; i++) {
			for (int j = 0; j < m; j++) {
				printf("%.2lf ", M[i][j]);
			}
			putchar('\n');
		}
		putchar('\n');
	}
}MAT;
struct layer { // Layer of the Network
	mat a, w, b, z;
	int id;
	layer() {}
	layer(int id) : id(id) {
		a = mat(N[id], 1);
		if (id) {
			w = mat(N[id], N[id-1]);
			b = mat(N[id], 1);
		}
	}
}baseNet[L]; // basic network
db getrand() { return 1.0 * rand() / RAND_MAX; }
void init() { // initialize Training Data
	freopen("train.in", "r", stdin);
	for (int i = 0; i < T; i++) {
		for (int j = 0; j < IN; j++) {
			scanf("%lf", &image[i][j]);
			image[i][j] /= 255;
		}
	}
	fclose(stdin);
	freopen("train.out", "r", stdin);
	for (int i = 0; i < T; i++) scanf("%d", &ans[i]);
	fclose(stdin);
	printf("Reading complete!\n");
	// image Input TEST
	//for (int i = 0; i < 784; i++) {
	//	printf("%d ", (int)(image[0][i] * 255));
	//	if ((i+1) % 28 == 0) {
	//		putchar('\n');
	//	}
	//}
	freopen("diary.out", "w", stdout);
	fclose(stdout);
}
void Save(int num) { // Print Learning Result
	string s = string("Result") + to_string(num) + string(".out");
	freopen(s.c_str(), "w", stdout);
	printf("%d\n", L);
	for (int i = 0; i < L; i++) printf("%d ", N[i]);
	putchar('\n');
	for (int l = 1; l < L; l++) {
		baseNet[l].w.print();
		baseNet[l].b.print();
	}
	fclose(stdout);
}
// Back Propagation (Learning)
mat f(mat &x) { // activate function (sigmoid)
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (1 + exp(-t));
		}
	}
	return z;
}
mat _f(mat &x) { // Derivative of activate function 
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (exp(t) + exp(-t) + 2);
		}
	}
	return z;
}
// Id of Learning Data and Total gradient and cost
db BP(int id, layer grad[]) { // return Cost
	layer net[L];
	mat y(OUT, 1); // Desired result (Answer)
	for (int i = 0; i < L; i++) net[i] = baseNet[i];
	// initialize Input & Desired Data
	for (int i = 0; i < IN; i++) net[0].a.M[i][0] = image[id][i];
	for (int i = 0; i < OUT; i++)
		if (i == ans[id]) y.M[i][0] = 1;
	// Forward
	for (int l = 1; l < L; l++) {
		net[l].z = net[l].w * net[l-1].a + net[l].b;
		net[l].a = f(net[l].z);
	}
	// Backward
	mat dc_da = (net[L-1].a + (y * (-1))) * 2;
	for (int l = L-1; l >= 1; l--) {
		mat _fz = _f(net[l].z);
		mat dc_db = MAT.dot(dc_da, _fz);
		grad[l].b = grad[l].b + dc_db;
		grad[l].w = grad[l].w + (dc_db * (~net[l-1].a));
		dc_da = (~net[l].w) * dc_db;
	}
	// Cost
	db cost = 0;
	for (int i = 0; i < OUT; i++) cost += pow(net[L-1].a.M[i][0] - y.M[i][0], 2);
	return cost;
}
void GroupLearn(int st, vi *perm, layer grad[], db *cost) { // Thread of Group Learning with start id
	for (int j = st; j < st + GROUP; j++) { // assign Learning tasks
		*cost += BP((*perm)[j], grad);
	}
}
void ANN() { // Artificial Neural Network
	// initialize the struct of Network
	for (int i = 0; i < L; i++) baseNet[i] = layer(i);
	if (freopen("Result.in", "r", stdin) == NULL) { // initialize w and b randomly
		freopen("/dev/tty", "w", stdout);
		printf("Randomly initialization\n");
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++) {
				for (int j = 0; j < N[l-1]; j++)
					baseNet[l].w.M[i][j] = getrand() * 10 - 5;
				baseNet[l].b.M[i][0] = getrand() * 40 - 20;
			}
		}
	} else { // Using last Learning Data
		freopen("/dev/tty", "w", stdout);
		printf("Get Result.in\n");
		int rL;
		scanf("%d", &rL);
		assert(L == rL);
		vi rN(L);
		for (int i = 0; i < L; i++) {
			scanf("%d", &rN[i]);
			assert(rN[i] == N[i]);
		}
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++)
				for (int j = 0; j < N[l-1]; j++)
					scanf("%lf", &baseNet[l].w.M[i][j]);
			for (int i = 0; i < N[l]; i++)
				scanf("%lf", &baseNet[l].b.M[i][0]);
		}
		fclose(stdin);
	}
	vi perm(T);
	for (int i = 0; i < T; i++) perm[i] = i;
	int fg = 0; // id of Save data
	for (int _i = 0; _i < TOT; _i++) {
		random_shuffle(perm.begin(), perm.end());
		db Cost = 0;
		for (int i = 0; i < GROUP * NUM; i += GROUP * THR) {
			layer grad[THR][L]; // average gradient of a group for each thread
			db cost[THR] = {0}; // average cost of a group
			thread th[THR];
			for (int t = 0; t < THR; t++)
				for (int l = 0; l < L; l++)
					grad[t][l] = layer(l);
			for (int t = 0; t < THR; t++) {
				th[t] = thread(GroupLearn, i + t * GROUP, &perm, grad[t], &cost[t]);
			}
			for (int t = 0; t < THR; t++) {
				th[t].join();
			}
			for (int i = 1; i < L; i++) { // Upgrade Network
				for (int t = 0; t < THR; t++) {
					baseNet[i].w = baseNet[i].w + grad[t][i].w * (-1.0 / GROUP);
					baseNet[i].b = baseNet[i].b + grad[t][i].b * (-1.0 / GROUP);
				}
			}
			for (int t = 0; t < THR; t++) {
				Cost += cost[t] / GROUP;
			}
		}
		Save(fg);
		fg ^= 1;
		freopen("diary.out", "a", stdout);
		printf("%lf\n", Cost / (T / GROUP));
		fclose(stdout);
		freopen("/dev/tty", "w", stdout);
		printf("complete turn: %d\n", _i+1);
	}
	// TEST
	//for (int i = 0; i < 10; i++) {
	//	layer grad[L]; // average gradient of the group
	//	for (int l = 0; l < L; l++) grad[l] = layer(l);
	//	for (int j = i; j < i + 1; j++) { // assign Learning tasks
	//		db cost = BP(perm[0], grad);
	//		printf("%lf\n", cost);
	//	}
	//	for (int i = 1; i < L; i++) { // Upgrade Network
	//		baseNet[i].w = baseNet[i].w + grad[i].w * (-1.0 / GROUP);
	//		baseNet[i].b = baseNet[i].b + grad[i].b * (-1.0 / GROUP);
	//	}
	//}
}
signed main() {
	srand(time(NULL));
	init();
	clock_t st = clock(), en;
	ANN();
	en = clock();
	freopen("diary.out", "a", stdout);
	printf("Learning time: %lf s\n", 1.0 * (en - st) / CLOCKS_PER_SEC);
	fclose(stdout);
	return 0;
}
```

æœ€åè¿˜æœ‰ä¸€ä»½ä»£ç `ANN_Check.cpp`ï¼Œç”¨äºæµ‹è¯•å­¦ä¹ æ•ˆæœçš„ï¼Œç”¨äºæ£€éªŒå½“å‰ç½‘ç»œçš„æµ‹è¯•æ•°æ®çš„æ­£ç¡®æ€§

```c++
#include <bits/stdc++.h>
#define db double
#define ll long long
#define vi vector<int>
#define vii vector<vi >
#define vd vector<db>
#define vdd vector<vd >
#define pii pair<int, int>
#define pdd pair<db, db>
#define vpd vector<pdd >
#define vipd vector<vpd >
#define vp vector<pii >
#define vip vector<vp >
#define mkp make_pair
#define pb push_back
using namespace std;
const int INF = 0x3f3f3f3f;
const int T = 10000; // Number of Total training Data
const int L = 4; // Number of Layers (contains Input layer and Output layer)
const int IN = 784; // Number of Nodes in Layer 1 (Input Layer)
const int OUT = 10; // Number of Nodes in Layer L-1 (Output Layer)
const int N[L] = {IN, 16, 16, OUT}; // Number of Nodes in each Layer
//vd N(L); 
db image[T][IN]; // Image Data
int ans[T]; // Label of Image Data (Answer)
struct mat{ // Matrix Data Struct
	int n, m; // Size of Matrix : n * m
	vdd M;
	mat() {}
	mat(int n, int m, int num = 0) : n(n), m(m) { M = vdd(n, vd(m, num)); }
	mat operator * (const mat &y) const & { // multiply of Matrix
		assert(m == y.n);
		mat z(n, y.m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < y.m; j++)
				for (int k = 0; k < m; k++)
					z.M[i][j] += M[i][k] * y.M[k][j];
		return z;
	}
	mat operator + (const mat &y) const & { // addition of Matrix
		assert(n == y.n && m == y.m);
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] + y.M[i][j];
		return z;
	}
	mat operator * (const double &y) const & { // multiply Matrix and Const
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = M[i][j] * y;
		return z;
	}
	mat dot(mat &x, mat &y) { // dot multiplay of Matrix
		assert(x.n == y.n && x.m == y.m);
		int n = x.n, m = x.m;
		mat z(n, m);
		for (int i = 0; i < n; i++)
			for (int j = 0; j < m; j++)
				z.M[i][j] = x.M[i][j] * y.M[i][j];
		return z;
	}
	mat operator ~ () const & { // transpose the Matrix
		mat z(m, n);
		for (int i = 0; i < m; i++)
			for (int j = 0; j < n; j++)
				z.M[i][j] = M[j][i];
		return z;
	}
	void print() { // print the Matrix
		for (int i = 0; i < n; i++) {
			for (int j = 0; j < m; j++) {
				printf("%.2lf ", M[i][j]);
			}
			putchar('\n');
		}
		putchar('\n');
	}
}MAT;
struct layer { // Layer of the Network
	mat a, w, b, z;
	int id;
	layer() {}
	layer(int id) : id(id) {
		a = mat(N[id], 1);
		if (id) {
			w = mat(N[id], N[id-1]);
			b = mat(N[id], 1);
		}
	}
}baseNet[L]; // basic network
db getrand() { return 1.0 * rand() / RAND_MAX; }
void init() { // initialize Training Data
	freopen("test.in", "r", stdin);
	for (int i = 0; i < T; i++) {
		for (int j = 0; j < IN; j++) {
			scanf("%lf", &image[i][j]);
			image[i][j] /= 255;
		}
	}
	fclose(stdin);
	freopen("test.out", "r", stdin);
	for (int i = 0; i < T; i++) scanf("%d", &ans[i]);
	fclose(stdin);
	printf("Reading complete!\n");
	// image Input TEST
	//for (int i = 0; i < 784; i++) {
	//	printf("%d ", (int)(image[0][i] * 255));
	//	if ((i+1) % 28 == 0) {
	//		putchar('\n');
	//	}
	//}
}
mat f(mat &x) { // activate function (sigmoid)
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (1 + exp(-t));
		}
	}
	return z;
}
mat _f(mat &x) { // Derivative of activate function 
	mat z(x.n, x.m);
	for (int i = 0; i < x.n; i++) {
		for (int j = 0; j < x.m; j++) {
			db t = x.M[i][j];
			z.M[i][j] = 1.0 / (exp(t) + exp(-t) + 2);
		}
	}
	return z;
}
// Id of Checking Data
int CK(int id) { // return Output
	layer net[L];
	for (int i = 0; i < L; i++) net[i] = baseNet[i];
	// initialize Input & Desired Data
	for (int i = 0; i < IN; i++) net[0].a.M[i][0] = image[id][i];
	// Forward
	for (int l = 1; l < L; l++) {
		net[l].z = net[l].w * net[l-1].a + net[l].b;
		net[l].a = f(net[l].z);
	}
	double mx = 0;
	int out;
	for (int i = 0; i < OUT; i++) {
		if (net[L-1].a.M[i][0] > mx) {
			mx = net[L-1].a.M[i][0];
			out = i;
		}
	}
	return out;
}
void ANN() { // Artificial Neural Network
	// initialize the struct of Network
	for (int i = 0; i < L; i++) baseNet[i] = layer(i);
	if (freopen("Result.in", "r", stdin) == NULL) { // initialize w and b randomly
		freopen("/dev/tty", "w", stdout);
		printf("Randomly initialization\n");
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++) {
				for (int j = 0; j < N[l-1]; j++)
					baseNet[l].w.M[i][j] = getrand() * 10 - 5;
				baseNet[l].b.M[i][0] = getrand() * 40 - 20;
			}
		}
	} else { // Using last Learning Data
		freopen("/dev/tty", "w", stdout);
		printf("Get Result.in\n");
		int rL;
		scanf("%d", &rL);
		assert(L == rL);
		vi rN(L);
		for (int i = 0; i < L; i++) {
			scanf("%d", &rN[i]);
			assert(rN[i] == N[i]);
		}
		for (int l = 1; l < L; l++) {
			for (int i = 0; i < N[l]; i++)
				for (int j = 0; j < N[l-1]; j++)
					scanf("%lf", &baseNet[l].w.M[i][j]);
			for (int i = 0; i < N[l]; i++)
				scanf("%lf", &baseNet[l].b.M[i][0]);
		}
		fclose(stdin);
	}
	int yes = 0;
	vi perm(T);
	for (int i = 0; i < T; i++) perm[i] = i;
	random_shuffle(perm.begin(), perm.end());
	for (int i = 0; i < T; i++) {
		if (CK(i) == ans[i]) yes++;
	}
	freopen("/dev/tty", "w", stdout);
	printf("%lf", 1.0 * yes / T);
	// TEST
	//for (int i = 0; i < 10; i++) {
	//	layer grad[L]; // average gradient of the group
	//	for (int l = 0; l < L; l++) grad[l] = layer(l);
	//	for (int j = i; j < i + 1; j++) { // assign Learning tasks
	//		db cost = BP(perm[0], grad);
	//		printf("%lf\n", cost);
	//	}
	//	for (int i = 1; i < L; i++) { // Upgrade Network
	//		baseNet[i].w = baseNet[i].w + grad[i].w * (-1.0 / GROUP);
	//		baseNet[i].b = baseNet[i].b + grad[i].b * (-1.0 / GROUP);
	//	}
	//}
}
signed main() {
	srand(time(NULL));
	init();
	ANN();
	return 0;
}
```

ï¼ˆæˆ‘çš„ä»£ç æŠ˜å å™¨åäº†ï¼Œåªèƒ½å…ˆè¿™æ ·äº†ğŸ˜¢ï¼‰

#### å­¦ä¹ æ•ˆæœ

ç»è¿‡å¤šçº¿ç¨‹è®¡ç®—ï¼Œ5håç¬¬ä¸€ç»„æ•°æ®åŸºæœ¬æ”¶æ•›äº†ï¼Œæœ€åçš„æ­£ç¡®ç‡åˆ°è¾¾ `81%`ï¼ˆyysyç¬¬ä¸€æ¬¡èƒ½åˆ°è¿™ä¸ªæ­£ç¡®çš„ï¼Œæˆ‘è§‰å¾—è¿˜è¡Œäº†ï¼‰ï¼Œè€Œåˆ«äººåšçš„å¯ä»¥åˆ°è¾¾ `90%` ä»¥ä¸Šï¼Œæœ€è¿‘å‡ å¤©è¿˜åœ¨è®¡ç®—ä¸­ï¼Œå¸Œæœ›èƒ½æœ‰æ‰€æé«˜ã€‚
