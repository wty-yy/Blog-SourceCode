---
title: TensorFlow - ä½¿ç”¨GradientTapeå’Œé‡å†™fitè®­ç»ƒç»“æœä¸åŒçš„åŸå› 
hide: false
math: true
abbrlink: 4237
date: 2022-11-23 12:52:47
index\_img:
banner\_img:
category:
 - TensorFlow2
tags:
 - MNIST
---

> è¯¥é—®é¢˜æ˜¯åœ¨ä½¿ç”¨GradientTapeè®­ç»ƒMNISTæ•°æ®é›†æ—¶å‘ç°çš„ï¼Œå°è¯•ä½¿ç”¨äº†ä¸‰ç§æ–¹å¼è¿›è¡Œè®­ç»ƒï¼šç›´æ¥GradientTapeè®­ç»ƒï¼Œè°ƒç”¨fitå‡½æ•°è®­ç»ƒï¼Œé‡å†™fitå‡½æ•°åè®­ç»ƒ. å‘ç°é‡å†™GradientTapeè®­ç»ƒçš„æ­£ç¡®ç‡å°½ç„¶æœ‰96%ï¼Œè€Œåä¸¤è€…çš„æ­£ç¡®ç‡90%éƒ½ä¸åˆ°ï¼Œè¿™å¼•èµ·äº†æˆ‘å¾ˆå¤§çš„å¥½å¥‡å¿ƒï¼Œäºæ˜¯é€šè¿‡æŸ¥é˜…å¤§é‡æ–‡æ¡£å’Œé˜…è¯»TFæºä»£ç ä¸€æ­¥ä¸€æ­¥æ’é™¤é—®é¢˜ï¼Œæœ€ç»ˆæ‰¾åˆ°é—®é¢˜åŸå› .

è®­ç»ƒé›†ä½¿ç”¨æœ€ç®€å•çš„MNISTï¼Œé‡å†™fitå‡½æ•°éƒ¨åˆ†å‚è€ƒï¼š[Keyird - 1. æ‰‹å†™æ•°å­—è¯†åˆ«](https://github.com/Keyird/DeepLearning-TensorFlow2/blob/master/1.%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/FirstNet.py)

# ä¸‰ç§åŸºç¡€æ¨¡å‹è®­ç»ƒæ–¹æ³•

batchå¤§å°ç»Ÿä¸€ä¸º $32$ï¼Œepochä¸ªæ•°ä¸º $10$ï¼Œä¼˜åŒ–å™¨å‡é‡‡ç”¨ `keras.optimizers.SGD(lr=0.01)`ï¼ˆå­¦ä¹ ç‡ä¸º0.01ï¼‰ï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±ï¼Œç½‘ç»œç»“æ„å¦‚ä¸‹

```python
dense_network = Sequential([
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(10)
])
dense_network.build(input_shape=(None, 28*28))
```

## GradientTapeè®­ç»ƒæ–¹æ³•

é¦–å…ˆå‚è€ƒKeyirdä½¿ç”¨GradientTapeè®­ç»ƒçš„æ–¹æ³•ï¼š

è¿™é‡Œå°½å¯èƒ½ä½¿ç”¨ `tf` ç±»ä¸­çš„å‡½æ•°ï¼Œå› ä¸ºå…¶å‡½æ•°å¤§å¤šéƒ½æœ‰ä¼˜åŒ–ï¼Œæœ¬æ¬¡å­¦åˆ°çš„æ–°åŠŸèƒ½æœ‰ï¼š

1. ä½¿ç”¨ `tf.constant` æˆ– `tf.convert_to_tensor` å°†æ•°æ®è½¬åŒ–ä¸º `tf.tensor` æ•°æ®ç±»å‹ï¼Œè¿™ç§ç±»å‹ç±»ä¼¼äº `np.ndarray`ï¼Œæ‰€ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥èµ·åˆ°æ›¿ä»£ä½œç”¨.

2. `dense_network.build(input_shape=(None, 28*28))`ï¼Œè®¾ç½®è¾“å…¥ç‰¹å¾ä¸º28\*28ï¼Œè€Œä¸æ˜¯åƒä»¥å¾€å°† `input_shape` å†™åˆ° `Sequential` ç½‘ç»œçš„ç¬¬ä¸€å±‚ä¸­.

3. éªŒè¯é›†ä¹Ÿå¯ä»¥æŒ‰ç…§batchè¿›è¡Œåˆ†å‰²ï¼Œæ¯æ¬¡åˆ¤æ–­ä¸€æ•´ä¸ªbatchçš„æ­£ç¡®ç‡å³å¯ï¼Œè€Œä¸”batchè¶Šå¤§ï¼Œé¢„å¤„ç†é€Ÿåº¦è¶Šå¿«ï¼Œæ‰€ä»¥å¯ä»¥å°è¯•å°†æ•´ä¸ªéªŒè¯é›†æ”¾åˆ°ä¸€ä¸ªbatchä¸­ï¼Œå‡å°‘äº†æ•°æ®è¯»å…¥èŠ±è´¹çš„æ—¶é—´ï¼š`test_ds = test_ds.batch(len(test_ds))`.

4. ä½¿ç”¨TFè‡ªå¸¦çš„è®°å½•å™¨æ›´å®¹æ˜“åˆ¤æ–­ç»“æœçš„å‡†ç¡®ç‡ï¼š[`tf.metrics.Accuracy()`](https://tensorflow.google.cn/api_docs/python/tf/keras/metrics/Accuracy?hl=en) æ˜¯æœ€ç®€çš„ä¸€ç§å‡†ç¡®ç‡æµ‹é‡å™¨ï¼Œå¯ä»¥ç”¨äºæ¯”å¯¹å¯¹åº”æ ‡ç­¾æ˜¯å¦ç›¸åŒ. **å¥½ç”¨ä½†æ˜¯ä¸€å®šè¦ç”¨å¯¹ï¼Œä¸¤ç§å†™æ³•ä¸Šæ­£ç¡®ç‡é—®é¢˜å°±å‡ºåœ¨è¿™é‡Œï¼Œä¸‹æ–‡ä¼šè¯¦ç»†ä»‹ç»è®°å½•å™¨çš„ä½¿ç”¨æ–¹æ³•**.

{% spoiler "å®Œæ•´GradientTapeä»£ç "%}
```python
# coding: utf-8
import tensorflow as tf
from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # å¯¼å…¥å­åº“
from tqdm import tqdm

# æ•°æ®é›†è¯»å…¥
(x, y), (x_val, y_val) = datasets.mnist.load_data()
x = tf.constant(x, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y = tf.constant(y, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾
x_val = tf.constant(x_val, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y_val = tf.constant(y_val, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾

train_ds = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºæ•°æ®å¯¹è±¡
test_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))  # æ„å»ºæ•°æ®å¯¹è±¡
test_ds = test_ds.batch(len(test_ds))  # å°†éªŒè¯é›†æ‰“åŒ…ä¸ºä¸€ä¸ªbatchï¼Œé¢„æµ‹é€Ÿåº¦å¤§å¤§å¢åŠ 
train_ds = train_ds.shuffle(1000).batch(32).repeat(10)  # æ‰“ä¹±æ•°æ®é›†ï¼Œè®¾ç½®è®­ç»ƒbatchä¸º32ï¼Œé‡å¤10é
batch_N = len(train_ds)

# 2. ç½‘ç»œæ­å»º
dense_network = Sequential([
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(10)
])
dense_network.build(input_shape=(None, 28*28))  # è®¾ç½®è¾“å…¥ç‰¹å¾ä¸º28*28

# 3.æ¨¡å‹è®­ç»ƒ
optimizer = optimizers.SGD(lr=0.01)  # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå­¦ä¹ ç‡=0.01
acc_meter = metrics.Accuracy()  # å‡†ç¡®ç‡æµ‹é‡å™¨ï¼Œç´¯è®¡è®°å½•å™¨
for step, (x, y) in enumerate(train_ds):  # è¾“å…¥ä¸€ä¸ªbatchæ•°æ®è¿›è¡Œè®­ç»ƒ
    with tf.GradientTape() as tape:  # æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
        out = dense_network(x)  # è¾“å‡º [b,10]
        y_onehot = tf.one_hot(y, depth=10)  # one-hotç¼–ç 
        loss = tf.reduce_sum(tf.square(out - y_onehot))/32.  # å‡æ–¹æŸå¤±å‡½æ•°
    grads = tape.gradient(loss, dense_network.trainable_variables)  # æ±‚losså…³äºç½‘ç»œä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦
    optimizer.apply_gradients(zip(grads, dense_network.trainable_variables))  # æ›´æ–°ç½‘ç»œå‚æ•°
    acc_meter.update_state(y_true=y, y_pred=tf.argmax(out, axis=1))  # æ¯”è¾ƒé¢„æµ‹å€¼ä¸æ ‡ç­¾ï¼Œæ›´æ–°å‡†ç¡®ç‡
    if step % 200 == 0:  # æ¯200ä¸ªstepè¾“å‡ºä¸€æ¬¡ç»“æœ
        print(f"{'step='+str(step)+'/'+str(batch_N):<20} loss={loss.numpy():.4f}\
                 Accuracy={acc_meter.result().numpy():.4f}")
        acc_meter.reset_states()  # å‡†ç¡®ç‡æ¸…ç©º

# 4.éªŒè¯é›†é¢„æµ‹
pred_meter = metrics.Accuracy()  # å‡†ç¡®ç‡æµ‹é‡å™¨
for x, y in tqdm(test_ds):
    out = dense_network(x)  # è¾“å‡º [b,10]
    pred = tf.argmax(out, axis=1)  # é¢„æµ‹ç»“æœ[b,]
    pred_meter(y, pred)
print('éªŒè¯é›†å‡†ç¡®ç‡', pred_meter.result().numpy())
```
{% endspoiler %}

è¿™ç§å†™æ³•è®­ç»ƒé›†ä¸Šçš„æ­£ç¡®ç‡ä¸º 96.86%ï¼ŒéªŒè¯é›†ä¸Šçš„æ­£ç¡®ç‡ä¸º 96.61%. ï¼ˆé€‰ç”¨æ›´å°çš„å­¦ä¹ ç‡å¯ä»¥è¿›ä¸€æ­¥æé«˜åˆ°98%ï¼‰

## ç›´æ¥è°ƒç”¨fitå‡½æ•°

è¯¥éƒ¨åˆ†ä»£ç ä¸»è¦åœ¨æ•°æ®çš„æ ‡ç­¾ä¸Šåšäº†äº›è°ƒæ•´ï¼š

```python
y = tf.one_hot(y, depth=10)  # è½¬åŒ–ä¸ºone-hotç¼–ç 
y_evl = tf.one_hot(y_evl, depth=10)  # è½¬åŒ–ä¸ºone-hotç¼–ç 
```

å…¶ä½™éƒ¨åˆ†ä¸ºè¶…å‚æ•°é…ç½®ï¼Œç›´æ¥è®­ç»ƒå³å¯ï¼Œè¯¦ç»†è¯·è§ä»£ç 

{% spoiler "ç›´æ¥è°ƒç”¨fitä»£ç "%}
```python
# coding: utf-8
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # å¯¼å…¥å­åº“

(x, y), (x_evl, y_evl) = datasets.mnist.load_data()

# è®­ç»ƒé›†
x = tf.constant(x, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y = tf.constant(y, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾
y = tf.one_hot(y, depth=10)  # è½¬åŒ–ä¸ºone-hotç¼–ç 
train_ds = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºæ•°æ®å¯¹è±¡
train_ds = train_ds.shuffle(1000).batch(32)  # æ‰“ä¹±æ•°æ®é›†ï¼Œè®¾ç½®è®­ç»ƒbatchä¸º32

# éªŒè¯é›†
x_evl = tf.constant(x_evl, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y_evl = tf.constant(y_evl, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾
y_evl = tf.one_hot(y_evl, depth=10)  # è½¬åŒ–ä¸ºone-hotç¼–ç 
evl_ds = tf.data.Dataset.from_tensor_slices((x_evl, y_evl)).batch(32)  # æ„å»ºæ•°æ®å¯¹è±¡

# æ„å»ºç½‘ç»œæ¡†æ¶
dense_network = Sequential([
    layers.Reshape(target_shape=(28*28,), input_shape=(28, 28)),
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(10)
])
dense_network.build(input_shape=(None, 28*28))  # è®¾ç½®è¾“å…¥ç‰¹å¾ä¸º28*28

# è®¾å®šè¶…å‚æ•°è¿›è¡Œè®­ç»ƒ
optimizer = optimizers.SGD(lr=0.01)  # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå­¦ä¹ ç‡=0.01
dense_network.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])
dense_network.fit(x, y, epochs=10, validation_data=evl_ds)

print(dense_network.evaluate(evl_ds))  # ç›´æ¥è°ƒç”¨è¯„ä¼°å‡½æ•°

# æ‰‹å†™éªŒè¯é›†é¢„æµ‹
meter = metrics.Accuracy()
for x, y in evl_ds:
    out = dense_network(x)  # è¾“å‡º [b,10]
    pred = tf.argmax(out, axis=1)
    y = tf.argmax(y, axis=1)
    meter.update_state(y, pred)
print('éªŒè¯é›†å‡†ç¡®ç‡', meter.result())
```
{% endspoiler %}

è¿è¡Œä»£ç å‘ç°è®­ç»ƒé›†æ­£ç¡®ç‡ä¸º 92%ï¼ŒéªŒè¯é›†æ­£ç¡®ç‡ä¸º93%.

### å‘ç°é—®é¢˜

ç›´æ¥è°ƒç”¨fitå‡½æ•°ä»£ç çœ‹ä¸Šå»å¾ˆç®€å•ï¼Œä½†æ˜¯å‡ºç°äº†éå¸¸å¤§çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯è®­ç»ƒå‡ºæ¥çš„å‡†ç¡®ç‡å’Œlosså‡½æ•°å€¼å®Œå…¨ä¸åŒï¼ï¼ï¼è€Œä¸”å‡†ç¡®ç‡è¿œä½äºç›´æ¥ä½¿ç”¨GradientTapeçš„å†™æ³•ï¼Œlosså‡½æ•°å€¼ä¹Ÿä¸ç›¸åŒ. **ç›´æ¥ä½¿ç”¨fitçš„losså‡½æ•°å€¼åŸºæœ¬åœ¨0.01çš„æ•°é‡çº§ï¼Œè€ŒGradientTapeçš„losså‡½æ•°å€¼åœ¨0.1å·¦å³**.

è€Œä¸”ï¼Œæœ€åæ‰‹å†™éªŒè¯é›†é¢„æµ‹çš„æ­£ç¡®ç‡å’Œç›´æ¥è°ƒç”¨è¯„ä¼°å‡½æ•°çš„æ­£ç¡®ç‡ç›¸åŒï¼Œè¯´æ˜å‡†ç¡®ç‡çš„è®¡ç®—æ²¡æœ‰é—®é¢˜.

æ‰€ä»¥ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šæ˜¯å¦æ˜¯TFè‡ªå¸¦æŸå¤±å‡½æ•°MSEå‡ºäº†é—®é¢˜.

---

### è§£å†³é—®é¢˜

æˆ‘ä»¬ç›´æ¥è€ƒè™‘ä»€ä¹ˆæ˜¯MSEçš„å·¥ä½œåŸç†ï¼Œè¿›è¡Œå°è¯•å¯å¾—ä¸‹é¢ä¸¤ä¸ªç›¸ç­‰åŠå®˜æ–¹æ–‡æ¡£å¯çŸ¥ ï¼š

```python
a = tf.keras.losses.MSE([1,2,3],[0.5,1,1]).numpy()
b = (0.5**2+1**2+2**2) / 3
loss = mean(square(y_true - y_pred), axis=-1)  # å®˜æ–¹æ–‡æ¡£
```

é‚£ä¹Ÿå°±æ˜¯å‡è®¾ä¸¤ä¸ªå‘é‡ $\boldsymbol{y},\hat{\boldsymbol{y}}$ çš„ç»´æ•°å‡ä¸º $N$ï¼Œåˆ™ ï¼ˆ$||\cdot||_2$è¡¨ç¤ºæ¬§æ°è·ç¦»ï¼‰

$$
MSE(\boldsymbol{y},\hat{\boldsymbol{y}}) = \frac{1}{N}||\boldsymbol{y}-\hat{\boldsymbol{y}}||_2^2
$$

ç”±äºæŸå¤±å‡½æ•°åªä¼šä½œç”¨åœ¨è¾“å‡ºå‘é‡çš„æœ€åä¸€ç»´ä¸Šï¼Œä¹Ÿå°±æ˜¯è¾“å‡ºå‘é‡ç»´æ•°ä¸º `[b,10]`ï¼ˆ`b` è¡¨ç¤ºbatch_sizeï¼‰ï¼Œé‚£ä¹ˆ `MSE` è¿”å›å€¼å°±æ˜¯ `[b,]` çš„å‘é‡ï¼Œè€Œä¸æ˜¯æ ‡é‡. ä½†æ˜¯æˆ‘ä»¬éœ€è¦çš„æœ€ç»ˆæŸå¤±æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå³è¿™ä¸ªæŸå¤±çš„æœŸæœ›ï¼Œä¹Ÿå°±æ˜¯**æœŸæœ›é£é™©**ï¼Œå¦‚ä¸‹å®šä¹‰

$$
\mathcal{R}(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \frac{1}{b}\sum_{i=1}^b(\boldsymbol{y}_i-\hat{\boldsymbol{y}}_i)^2 = \frac{1}{b}\sum_{i=1}^bMSE(\boldsymbol{y},\hat{\boldsymbol{y}})\times 10
$$

è¿™é‡Œä¹˜ä»¥ $10$ çš„åŸå› æ˜¯ `MSE` é”™è¯¯è®¡ç®—äº†å‡å€¼ï¼Œé™¤ä»¥äº†10ï¼Œæ‰€ä»¥éœ€è¦è¿”å›ä¸€ä¸ª10.

æˆ‘ä»¬å¯¹ä¸Šè¿°çŒœæµ‹è¿›è¡ŒéªŒè¯ï¼Œåœ¨GradientTapeçš„ç¬¬35è¡Œè¿›è¡Œä¿®æ”¹

```python
loss1 = tf.reduce_sum(tf.square(out - y_onehot))/32.  # å‡æ–¹æŸå¤±å‡½æ•°
loss2 = tf.keras.losses.MSE(y_true=y_onehot, y_pred=out)  # æˆ–è€…ç›´æ¥ä½¿ç”¨kerasçš„MSE
loss2 = tf.reduce_sum(loss2) / 32. * 10.
print(loss1, loss2)  # ç»“æœä¸€è‡´
```

è¿™é‡Œé”™è¯¯çš„åŸå› æ˜¯ï¼šæœ€åçš„è¾“å‡ºå±‚æ˜¯ä¸€ä¸ª10ç»´å‘é‡ï¼Œè€ŒMSEæ¥å—çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ª1ç»´çš„æ•°å€¼ï¼Œè¿™æ ·æ‰èƒ½ä¿è¯è®¡ç®—ç»“æœçš„æ­£ç¡®æ€§. ä½†æ˜¾ç„¶é¢„æµ‹ä¸€ä¸ªæ•°å­—æ•ˆç‡æ˜¯éå¸¸ä½çš„ï¼ˆæ ‡ç­¾çš„å‡å€¼å¤ªå¤§ï¼Œç½‘ç»œè®­ç»ƒé€Ÿåº¦éå¸¸æ…¢ï¼‰ï¼Œåº”è¯¥è½¬åŒ–ä¸ºone-hotç¼–ç è¿›è¡Œé¢„æµ‹ï¼Œè€ŒMSEå¯¹one-hotç¼–ç å€¼åªä¼šè®¡ç®—æœ€åä¸€ç»´ï¼Œæ‰€ä»¥ç›´æ¥åœ¨fitå‡½æ•°ä¸­ä½¿ç”¨MSEæ˜¯æœ‰é—®é¢˜çš„ï¼Œå…¶è®¡ç®—çš„æŸå¤±å‡½æ•°å€¼ä¼šæ¯”çœŸå®çš„MSEè®¡ç®—å‡ºçš„ç»“æœå°10å€

æœ‰ä¸¤ä¸ªç®€å•çš„æ–¹æ³•åº”å¯¹ï¼š

1. ç”±äºæŸå¤±å‡½æ•°å€¼å°10å€ï¼Œä¹Ÿå°±æ˜¯æ¢¯åº¦æ¯æ¬¡æ›´æ–°å°10å€ï¼Œæ‰€ä»¥åªéœ€è¦å°†å­¦ä¹ ç‡å¢å¤§10å€å³å¯ï¼Œä¿®æ”¹ `optimizer = optimizers.SGD(lr=0.1)` å†æ¬¡è®­ç»ƒå¯å¾—åˆ°æ­£ç¡®ç‡ä¸º 96% è§£å†³é—®é¢˜.

2. é‡å†™losså‡½æ•°ï¼ŒåŒæ ·å¯å¾—åˆ°æ­£ç¡®çš„ 96% æ­£ç¡®ç‡.ï¼ˆæ¨èä½¿ç”¨è¯¥æ–¹æ³•ï¼‰

```python
def my_MSE(y_true=None, y_pred=None):  # é‡å†™è®­ç»ƒå‡½æ•°
    return tf.reduce_sum(tf.square(y_true - y_pred)) / 32.

dense_network.compile(optimizer=optimizer, loss=my_MSE, metrics=['accuracy'])  # é¿å…ä½¿ç”¨'MSE'
```

ä¸ºäº†å¼„æ¸…æ¥šfitå‡½æ•°çš„åŸç†ï¼Œæˆ‘ä»¬å‘ç°losså‡½æ•°è¾“å‡ºçš„ç»“æœæ˜¯ä¸€ä¸ªå‘é‡ï¼Œäºæ˜¯fitå‡½æ•°åº”è¯¥æ˜¯åšäº†å‡å€¼å¤„ç†ï¼Œå†™ä¸ºGradientTapeå½¢å¼å¦‚ä¸‹

```python
loss = tf.keras.losses.MSE(y_true=y_onehot, y_pred=out)  # ä»…ä¿®æ”¹ç¬¬ä¸€ä¸ªä»£ç çš„35è¡Œ
loss = tf.reduce_mean(loss)
```

å¾—åˆ°çš„æ­£ç¡®ç‡ä¸º 93% ä¸ç›´æ¥è°ƒç”¨fitçš„æ­£ç¡®ç‡ç›¸åŒ

---

### æºä»£ç åˆ†æ

é˜…è¯»fitå‡½æ•°ä¸­è®¡ç®—lossçš„[æºä»£ç ](https://github.com/keras-team/keras/blob/v2.11.0/keras/engine/training.py#L1024)ï¼Œå‘ç°åœ¨è®­ç»ƒéƒ¨åˆ†ä½¿ç”¨train_stepè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­computer_losså‡½æ•°ç”¨äºè®¡ç®—lossï¼Œé»˜è®¤è°ƒç”¨complied_lossè¿”å›Model.compileæ—¶å®šä¹‰çš„lossï¼Œç„¶åcomplied_lossåˆæ˜¯ç”±LossesContainerç±»å°è£…ï¼Œè¿™ä¸ªå°è£…ç±»ä¸­ç¬¬[113è¡Œ](https://github.com/keras-team/keras/blob/e6784e4302c7b8cd116b74a784f4b78d60e83c26/keras/engine/compile_utils.py#L113)è¯´æ˜è®¡ç®—å‡ºçš„losså‘é‡åå‡ä¼šä½¿ç”¨`keras.metrics.Mean`è¿›è¡Œå‡å€¼å¤„ç†ï¼ŒåŒæ ·è¯´æ˜ä¸Šè¿°çŒœæµ‹æ­£ç¡®.

```python
# ç›´æ¥è°ƒç”¨fitè®¡ç®—lossçš„è°ƒç”¨å…³ç³»
Model.fit -> train_step -> compute_loss -> compiled_loss -> compile_utils.LossContainer(losså‡å€¼å¤„ç†) -> ...
# æ‰€ä»¥æˆ‘ä»¬æƒ³è¦é‡å†™losså‡½æ•°å¯ä»¥ç›´æ¥é‡è½½model.compute_losså‡½æ•°å³å¯
```

## é‡å†™fitå‡½æ•°

è¯¥æ–¹æ³•çš„å¥½å¤„åœ¨äºæˆ‘ä»¬å¯ä»¥åˆ©ç”¨fitè‡ªå¸¦çš„è®¸å¤šcallbackåŠŸèƒ½ï¼Œä¾‹å¦‚ï¼štensorboardåŠŸèƒ½ï¼Œç”¨äºå¯è§†åŒ–è®­ç»ƒç»“æœã€ç½‘ç»œæ¡†æ¶ï¼Œå¯ä»¥ä¾¿æ·çš„å±•ç¤ºä½ çš„æ¨¡å‹.

é‡å†™fitå‡½æ•°å‚è€ƒ [TFæŒ‡å— - è‡ªå®šä¹‰Model.fit å†…å®¹](https://tensorflow.google.cn/guide/keras/customizing_what_happens_in_fit)ï¼Œæˆ‘ä»¬åªéœ€é‡å†™ `train_step` å’Œ `test_step` å‡½æ•°ï¼Œåˆ†åˆ«å¯¹åº”è®­ç»ƒ `Model.fit` å’Œè¯„ä¼° `Model.evaluate` å‡½æ•°. åœ¨ `train_step` ä¸­å®Œæˆ `GradientTape` è¿‡ç¨‹å³å¯ï¼Œå¹¶è¿”å›losså‡½æ•°å’Œå‡†ç¡®ç‡çš„æµ‹é‡å³å¯.

åˆå§‹åŒ–éƒ¨åˆ†æ²¿ç”¨ç¬¬ä¸€ç§ `GradientTape` æœªå°†æ ‡ç­¾è½¬åŒ–ä¸ºone-hotç¼–ç å½¢å¼


{% spoiler "é‡å†™fitå‡½æ•°è®­ç»ƒä»£ç "%}
```python
# coding: utf-8
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # å¯¼å…¥å­åº“
from tqdm import tqdm

(x, y), (x_val, y_val) = datasets.mnist.load_data()
# è®­ç»ƒé›†
x = tf.constant(x, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y = tf.constant(y, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾
train_ds = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºæ•°æ®å¯¹è±¡
train_ds = train_ds.shuffle(1000).batch(32)  # æ‰“ä¹±æ•°æ®é›†ï¼Œè®¾ç½®è®­ç»ƒbatchä¸º32ï¼Œé‡å¤10é

# éªŒè¯é›†
x_evl = tf.constant(x_val, dtype=tf.float32)/255.  # è½¬åŒ–ä¸ºtensorï¼Œå›¾åƒç‰¹å¾ç¼©æ”¾ä¸º0~1
y_evl = tf.constant(y_val, dtype=tf.int32)  # è½¬åŒ–ä¸ºtensorï¼Œæ ‡ç­¾
evl_ds = tf.data.Dataset.from_tensor_slices((x_evl, y_evl)).batch(32)  # æ„å»ºæ•°æ®å¯¹è±¡

optimizer = optimizers.SGD(lr=0.01)  # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå­¦ä¹ ç‡=0.01
def my_loss(y_true=None, y_pred=None):  # è‡ªå®šä¹‰æŸå¤±å‡½æ•°
    return tf.reduce_sum(tf.square(y_true - y_pred))/32.

class my_Sequential(keras.Sequential):
    def __init__(self, layers=None, name='mySeq'):
        super().__init__(layers=layers, name=name)

    def train_step(self, data):
        x, y = data
        with tf.GradientTape() as tape:
            out = self(x, training=True)
            y_onehot = tf.one_hot(y, depth=10)
            loss = self.compiled_loss(y_onehot, out)  # ä½¿ç”¨Model.compile()ä¸­çš„æŸå¤±å‡½æ•°
        grads = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))  # æ›´æ–°æ¢¯åº¦
        self.compiled_metrics.update_state(y, out)  # ä½¿ç”¨Model.metrics()ä¸­çš„æµ‹é‡å™¨ï¼Œæ›´æ–°æ­£ç¡®ç‡
        return {m.name: m.result() for m in self.metrics}  # å°†è¿”å›çš„æµ‹é‡å€¼ç”¨mapæ‰“åŒ…

    def test_step(self, data):  # ä¸train_stepéƒ¨åˆ†å®Œå…¨ç±»ä¼¼ï¼Œåªæ˜¯å°‘äº†æ¢¯åº¦æ›´æ–°ï¼Œä¸ç”¨è®¡ç®—losså’Œgrads
        x, y = data
        out = self(x, training=False)
        y_onehot = tf.one_hot(y, depth=10)
        loss = self.compiled_loss(y_onehot, out)
        self.compiled_metrics.update_state(y, out)
        return {m.name: m.result() for m in self.metrics}

dense_network = my_Sequential([
    layers.Flatten(input_shape=(28,28), name='Input'),
    layers.Dense(256, activation='relu', name='Dense1'),
    layers.Dense(128, activation='relu', name='Dense2'),
    layers.Dense(10, name='Output')
])

tb_callback = tf.keras.callbacks.TensorBoard(log_dir=r"logs/sgd/", histogram_freq=1)  # ä½¿ç”¨tensorboardè®°å½•å›è°ƒä¿¡æ¯
dense_network.compile(optimizer=optimizer, loss=my_loss, metrics=['accuracy'])
dense_network.fit(train_ds, epochs=10, validation_data=evl_ds, callbacks=[tb_callback])  # åœ¨callbacksä¸­åŠ å…¥tb_callback

dense_network.evaluate(evl_ds, return_dict=True)  # éªŒè¯é›†é¢„æµ‹
```
{% endspoiler %}

è®­ç»ƒé›†æ­£ç¡®ç‡ä¸º 97.05%ï¼ŒéªŒè¯é›†æ­£ç¡®ç‡ä¸º 96.76%

### é‡åˆ°çš„é—®é¢˜

ä¸Šè¿°ä»£ç æ˜¯æ²¡æœ‰ä»»ä½•é—®é¢˜çš„ä»£ç ï¼Œåœ¨ä¹‹å‰çš„ç¼–å†™è¿‡ç¨‹ä¸­å‘ç°ä¸¤ä¸ªé—®é¢˜ï¼š

#### å‡†ç¡®ç‡è®¡ç®—å‡ºé”™

`Model.compile` ä¸­ `metrics` ä½¿ç”¨ `accuracy` å‚æ•°å¯¹åº”çš„æµ‹é‡å™¨åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

- å‚è€ƒ[Model.compileå®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/api_docs/python/tf/keras/Model#compile)ä¸­metricsä»‹ç»ï¼ŒTensorFlowä¼šè‡ªåŠ¨æ ¹æ®æ•°æ®é›†çš„æ ‡ç­¾å’Œæ¨¡å‹çš„è¾“å‡ºè‡ªåŠ¨é€‰æ‹©ä¸‰ç§ä¸åŒçš„å‡†ç¡®ç‡æµ‹é‡å™¨ï¼š`tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy`ï¼Œè¿™é‡Œæ˜¯å¤šç»´åˆ†ç±»ï¼Œåªæ³¨é‡ç¬¬äºŒå’Œç¬¬ä¸‰ç§. ç»è¿‡å°è¯•å‘ç°ï¼Œ**è‡ªåŠ¨é€‰æ‹©**åŸç†åº”è¯¥æŒ‡çš„æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨compiled_metricsæ—¶ä¼šç¡®å®šä¸‹æ¥ï¼Œç¡®å®šæ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼ˆåˆ†ç±»å™¨é»˜è®¤éƒ½æ˜¯æ¯”è¾ƒ `[b,*]` é™¤å»ç¬¬ä¸€ç»´ä»¥å¤–çš„ä¿¡æ¯ï¼Œé»˜è®¤ç¬¬ä¸€ç»´æ˜¯batch_sizeï¼‰
- ç¬¬äºŒä¸ª `CategoricalAccuracy` å¤šé¡¹-å¤šé¡¹æµ‹é‡å™¨ï¼Œé€šè¿‡æ¯”å¯¹ä¸¤ä¸ª**æ¦‚ç‡åˆ†å¸ƒçš„æœ€å¤§å€¼**æ˜¯å¦ç›¸åŒï¼Œå¦‚æœ `y_true,y_pred` æœ€åä¸€ç»´**éƒ½æ˜¯å‘é‡æ—¶é€‰æ‹©**.
- ç¬¬ä¸‰ä¸ª `SparseCategoricalAccuracy` å•é¡¹-å¤šé¡¹æµ‹é‡å™¨ï¼Œé€šè¿‡æ¯”å¯¹ä¸€ä¸ªæ ‡é‡æ˜¯å¦æ˜¯å¦ä¸€ä¸ª**æ¦‚ç‡åˆ†å¸ƒçš„æœ€å¤§å€¼**ï¼Œï¼Œå¦‚æœ `y_true` æœ€åä¸€ç»´æ˜¯**å¸¸é‡**ï¼Œ `y_pred` æœ€åä¸€ç»´æ˜¯**å‘é‡æ—¶é€‰æ‹©**.

å› ä¸ºæˆ‘ä»¬å‘ç°ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨ `CategoricalAccuracy` å¯¹çœŸå®çš„æ ‡ç­¾è®¡ç®—ç²¾åº¦å‡†ç¡®ç‡ä¼šå¤§å¤§ä¸‹é™ï¼Œå› ä¸ºä»–ä¼šå°†å…¨éƒ¨çš„æ ‡ç­¾ä½œä¸ºä¸€ä¸ªå‘é‡è¿›è¡Œæ¯”å¯¹ï¼Œä¹Ÿå°±æ˜¯æ¯”å¯¹æ•°ç›®åªæœ‰batchä¸ªï¼Œä¸ `Accuracy` æ¯”è¾ƒç»“æœå®Œå…¨ä¸åŒï¼š

```python
m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([1,2,3], [1,2,2])
print(f"å‡†ç¡®ç‡: {m.result().numpy():.2f}, æ€»æ ·æœ¬æ•°ç›®: {m.count.numpy()}")
# å‡†ç¡®ç‡: 0.00, æ€»æ ·æœ¬æ•°ç›®: 1.0
acc = tf.keras.metrics.Accuracy()
acc.update_state([1,2,3], [1,2,2])
print(f"å‡†ç¡®ç‡: {acc.result().numpy():.2f}, æ€»æ ·æœ¬æ•°ç›®: {acc.count.numpy()}")
# å‡†ç¡®ç‡: 0.67, æ€»æ ·æœ¬æ•°ç›®: 3.0
```

è¿™ç¬¬ä¸€ç§è®¡ç®—å‡ºçš„å‡†ç¡®ç‡å¤§å¤§ä½äºç¬¬äºŒç§ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°†

- `self.compiled_metrics.update_state(y, tf.argmax(out, axis=-1))` å¯¹åº” `CategoricallAccuracy` æµ‹é‡å™¨. æœ€ç»ˆæ­£ç¡®ç‡ä»…æœ‰ 91.79%ï¼Œè€Œä¸”é‡æ–°æ‰‹å†™æ­£ç¡®çš„æµ‹é‡å™¨ï¼Œå¯¹æ¨¡å‹å‡†ç¡®ç‡æµ‹é‡å¾—åˆ°æ­£ç¡®ç‡ä¸º 96% è¯´æ˜å°±æ˜¯æµ‹é‡å™¨ä½¿ç”¨é”™è¯¯. å› ä¸ºæœ€åæ¯”è¾ƒçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€ä»¥è¿™é‡Œçš„æ­£ç¡®å†™æ³•åº”è¯¥æ˜¯ `self.compiled_metrics.update_state(y_onehot, out)`ï¼Œ`y_true,y_pred` ç»´æ•°å‡ä¸º `[b,10], [b,10]`
- `self.compiled_metrics.update_state(y, out)` å¯¹åº” `SparseCategoricalAccuracy` æµ‹é‡å™¨. æœ€ç»ˆæ­£ç¡®ç‡ä¸º 96.73%ï¼Œæ²¡æœ‰é”™è¯¯. `[b,], [b,10]`

è¿™é‡Œé”™è¯¯çš„ä½¿ç”¨æµ‹é‡å™¨çš„ä¸»è¦åŸå› åœ¨äºï¼Œé»˜è®¤äº† `model.compile` ä¸­ `accuarcy` å‚æ•°ä¼šä½¿ç”¨ `Accuracy` åº¦é‡å™¨ï¼Œä½†æ˜¯ä»–ä½¿ç”¨äº† `CategoricalAccuracy` åº¦é‡å™¨ï¼Œè¿™æ˜¯ä»¥åè¦æ³¨æ„çš„é—®é¢˜ï¼Œå› ä¸ºç¥ç»ç½‘ç»œæœ€åä¸€å±‚çš„è¾“å‡ºä¸€èˆ¬ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€ä»¥å¯¹åº”çš„æµ‹é‡å™¨ä¸€èˆ¬éƒ½å…·æœ‰å¤„ç†æ¦‚ç‡åˆ†å¸ƒçš„è¿‡ç¨‹ï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢æ±‚é¢„æµ‹å€¼ `tf.argmax(out, axis=-1)`.

> ps. è°ƒè¯•fitå†…éƒ¨çš„å‡½æ•°ï¼Œè¦ä½¿ç”¨ `tf.print()` è¿›è¡Œè¾“å‡ºè°ƒè¯•ï¼Œå› ä¸ºæ•´ä¸ªè¿‡ç¨‹æ˜¯åˆ›å»ºåœ¨è®¡ç®—å›¾ä¸­çš„ï¼Œæ­£å¸¸ `print` å‡½æ•°æ— æ³•ç›´æ¥è¿›å…¥åˆ°è®¡ç®—å›¾ä¸­.

---

#### losså‡½æ•°è¾“å‡ºé—®é¢˜

ç¬¬äºŒä¸ªé—®é¢˜æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœä¸ä½¿ç”¨compiled_lossè®¡ç®—losså€¼ï¼Œç›´æ¥åœ¨è¾“å‡ºéƒ¨åˆ†è¿”å›è‡ªå·±è®¡ç®—å‡ºçš„losså€¼ï¼š

```python
def test_step(self, data):
    x, y = data
    out = self(x, training=False)
    y_onehot = tf.one_hot(y, depth=10)
    loss = tf.reduce_sum(tf.square(y_true - y_pred))/32.  # è‡ªå·±è®¡ç®—loss
    self.compiled_metrics.update_state(y, out)
    matrics = {m.name: m.result() for m in self.metrics}
    matrics['loss'] = loss  # åœ¨è¿”å›çš„è¾“å‡ºä¸­åŠ ä¸Šloss
    return matrics
```

ä¼šå‘ç°è¾“å‡ºçš„logä¸­éªŒè¯é›†çš„losså€¼æ€»å°äºè®­ç»ƒé›†çš„losså€¼ï¼Œè€Œä¸”ä¸æ˜¯å‡ ä¸ªæ•°é‡åŠå€æ•°çš„å…³ç³».

ä½†æ˜¯åœ¨evaluateä¸­çš„è¾“å‡ºæ—¥å¿—ä¸­çš„lossåˆæ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯æœ€ç»ˆè¿”å›çš„logå­—å…¸ä¸­lossåˆæ˜¯é”™è¯¯çš„.

é¦–å…ˆé˜…è¯»è®¡ç®—validation_dataæŸå¤±å€¼çš„è®¡ç®—æ–¹æ³•ï¼Œåœ¨[train.pyç¬¬1694è¡Œ](https://github.com/keras-team/keras/blob/e6784e4302c7b8cd116b74a784f4b78d60e83c26/keras/engine/training.py#L1694)å‘ç°ï¼Œè®¡ç®—validation_dataçš„æŸå¤±å€¼å°±æ˜¯ä»evaluateè¿”å›çš„logå­—å…¸ä¸­æ±‚å‡ºæ¥çš„. æ‰€ä»¥é—®é¢˜å‡ºåœ¨logå­—å…¸çš„å€¼äºè‡ªå®šä¹‰çš„losså€¼ä¸åŒçš„åŸå› ï¼ˆè¿™é‡Œæˆ‘çš„çŒœæµ‹æ˜¯ï¼Œç”±äºæ²¡æœ‰ä½¿ç”¨compiled_lossï¼Œæ‰€ä»¥TFå¯èƒ½è‡ªè¡Œè®¡ç®—å‡ºcompiled_losså¯¹åº”çš„losså€¼è¿”å›å›æ¥ï¼Œè€Œè¿™ä¸ªé»˜è®¤çš„losså‡½æ•°å°±æ­£å¥½æ˜¯MSEï¼Œæ‰€ä»¥lossçš„å‡å€¼ç‰¹åˆ«å°ï¼‰.

è¿™é‡Œå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰è‡ªå®šä¹‰lossçš„æµ‹é‡å™¨å¯¼è‡´çš„ï¼Œè€Œlossçš„æµ‹é‡å™¨ä¸€èˆ¬éƒ½æ˜¯å–å‡å€¼ï¼Œæ‰€ä»¥ä¸ºäº†ä¿æŒç®€å•ï¼Œæˆ‘ä»¬é€‰æ‹©é‡å†™losså‡½æ•°ï¼Œç„¶åå¯¼å…¥åˆ°Model.compileä¸­ï¼Œè¿™æ ·è¿˜æ˜¯èƒ½ä½¿ç”¨compiled_lossè®¡ç®—lossï¼Œè€Œä¸”æ— éœ€åœ¨train_stepä¸­è¿”å›losså€¼ï¼Œäºæ˜¯å°±æœ‰äº†ä¸Šè¿°çš„å†™æ³•.

### æ¨¡å‹å¯è§†åŒ–

æˆ‘ä»¬ä½¿ç”¨é‡å†™fitå‡½æ•°çš„ç›®æ ‡å°±æ˜¯ä¸ºäº†ä½¿ç”¨Tensorboardæ¥è¿›è¡Œæ¨¡å‹å¯è§†åŒ–ï¼Œå…·ä½“æ“ä½œéå¸¸ç®€å•ï¼Œå‚è€ƒ[TFæŒ‡å— - å¼€å§‹ä½¿ç”¨TensorBoard](https://tensorflow.google.cn/tensorboard/get_started)ï¼Œåªéœ€åœ¨ `Model.fit` çš„å›è°ƒé€‰é¡¹ `callbacks` ä¸­åŠ å…¥ `tf.keras.callbacks.TensorBoard(log_dir=dir_path, histogram_freq=1)` å³å¯åœ¨æ–‡ä»¶å¤¹ `dir_path` ä¸­æ‰¾åˆ°æ¨¡å‹ç”Ÿæˆçš„æ—¥å¿—ï¼Œå†ä½¿ç”¨cmdçª—å£è¾“å…¥  `tensorboard --logdir dir_path` å³å¯è¿è¡ŒTensorBoardï¼Œcmdä¸­ä¼šè¿”å›ä¸€ä¸ªç½‘å€ï¼Œä»ç½‘é¡µä¸­æ‰“å¼€å³å¯.

æˆ‘ä»¬è¿™é‡Œåˆ†åˆ«åˆ›å»ºä¸¤ä¸ªlog_dirè·¯å¾„ï¼Œåˆ†åˆ«å‘½åä¸º `"logs/sgd/"` å’Œ `"logs/adam/"` ç”¨äºæ¯”å¯¹ä¸¤ç§ä¼˜åŒ–å™¨ `SGD(lr=0.01)` å’Œ `Adam(lr=0.001)` çš„è®­ç»ƒç»“æœ.

![Accuracy](https://s1.ax1x.com/2022/11/24/zGBYE6.png)

![loss](https://s1.ax1x.com/2022/11/24/zGBtUK.png)

å¯ä»¥çœ‹å‡ºï¼Œ`adam` çš„è®­ç»ƒæ•ˆæœéå¸¸å¥½ï¼ŒåŒæ ·çš„è®­ç»ƒæ¬¡æ•°ä¸‹ï¼ŒéªŒè¯é›†éƒ½å¿«è¿‡æ‹Ÿåˆäº†ğŸ¤£ï¼Œè€Œä¸”lossä¸‹é™é€Ÿåº¦ä¹Ÿæ›´å¿«.

åœ¨æœ€ä¸Šé¢Graphsä¸€æ ä¸­ï¼Œæˆ‘ä»¬è¿˜èƒ½çœ‹åˆ°æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œéå¸¸ç›´è§‚

![Graphs](https://s1.ax1x.com/2022/11/24/zGBN4O.png)

æ”¾å¤§ `mySeq` ä¹Ÿå°±æ˜¯ç¥ç»ç½‘ç»œä¸»è¦ç»“æ„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ„å»ºçš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œéå¸¸ç›´è§‚

![Network Struct](https://s1.ax1x.com/2022/11/24/zGBGHx.png)

åœ¨Histogramsä¸€æ ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¯å­¦ä¹ å‚æ•°çš„ä¸»è¦åˆ†å¸ƒï¼Œå¯è§†åŒ–æ¨¡å‹å‚æ•°åˆ†å¸ƒ

![Histograms](https://s1.ax1x.com/2022/11/24/zGB6VP.png)

æ€»ä¹‹æœ¬æ¬¡è§£å†³é—®é¢˜å­¦ä¹ åˆ°äº†å¾ˆå¤šTFçš„å¯è‡ªå®šä¹‰å‡½æ•°ï¼Œä¾¿äºä»¥åè¿›è¡Œè‡ªå®šä¹‰æ¨¡å‹æ„å»ºï¼Œåº”è¯¥å…¨éƒ¨éƒ½ä¼šæŒ‰ç…§ç¬¬ä¸‰ç§é‡å†™fitå‡½æ•°çš„å½¢å¼è¿›è¡Œæ¨¡å‹è‡ªå®šä¹‰ï¼Œè¿™æ ·ä¹Ÿèƒ½ä¾¿äºå¯è§†åŒ–æ¨¡å‹ï¼Œååˆ†æ–¹ä¾¿ï¼
